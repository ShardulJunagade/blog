[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nML Publication Trends\n\n\n\nml publications\n\ndata analysis\n\ndata science\n\n\n\nAnalysis of Publication Trends in Top AI Conferences (2006-2024)\n\n\n\n\n\nApr 13, 2025\n\n\nShardul Junagade, Devansh Lodha\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Solar Panel Detector\n\n\n\ncode\n\nobject detection\n\ncomputer vision\n\n\n\nTraining a solar panel detector using YOLOv12 to detect solar panels in aerial images.\n\n\n\n\n\nFeb 27, 2025\n\n\nShardul Junagade\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nShardul Junagade\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html",
    "href": "posts/solar_panel_detector/solar_panel_detector.html",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "In this notebook, we will build a solar panel detector that can detect solar panels in aerial images. We’ll use the YOLOv12 model, which is the latest state-of-the-art object detection model from Ultralytics, to identify and locate solar panels in high-resolution aerial imagery.\nThis project will demonstrate:\n\nData preprocessing and exploration of the Solar Panel Object Labels dataset\nImplementation of evaluation metrics (IoU, AP)\nTraining and fine-tuning of the YOLOv12 model\nEvaluation of model performance on test data\nVisualization and interpretation of results\n\n\n\n\n\nCode\nimport torch\nfrom ultralytics import YOLO\nimport supervision as sv\nimport shapely.geometry as sg\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n\nUsing device: cuda\n\n\n\n\n\nWe will use the Solar Panel Object Labels dataset from Figshare. This dataset contains high-resolution aerial images with labeled solar panels. We will use the 31 cm native resolution images of sizes 416x416 pixels for our analysis.\nThe dataset files follow a specific naming structure: solarpanels_native_&lt;image_number&gt;__x0_&lt;x_coordinate&gt;_y0_&lt;y_coordinate&gt;_dxdy_&lt;size&gt;.\nFor example, in the file solarpanels_native_1__x0_0_y0_6845_dxdy_416.txt:\n\nsolarpanels_native: Indicates that the file contains solar panel data in native resolution.\n&lt;image_number&gt;: The number of the image in the dataset.\nx0_&lt;x_coordinate&gt;: The x-coordinate of the top-left corner of the image.\ny0_&lt;y_coordinate&gt;: The y-coordinate of the top-left corner of the image.\ndxdy_&lt;size&gt;: The size of the image in pixels (both width and height).\n\nEach line in the file represents a detected solar panel with the following format: category x_center y_center width height, where:\n\ncategory: The category label (0 for solar panels).\nx_center: The x-coordinate of the center of the bounding box (normalized).\ny_center: The y-coordinate of the center of the bounding box (normalized).\nwidth: The width of the bounding box (normalized).\nheight: The height of the bounding box (normalized).\n\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2553\nNumber of labels: 2542\n\n\nThe number of images and the number of labels is not the same. So, we can remove the images that do not have corresponding labels and remove the labels that do not have corresponding images.\n\n\nCode\n# delete the images that do not have corresponding labels\nfor image_name in image_names:\n    if image_name.replace('.tif', '.txt') not in label_names:\n        os.remove(image_dir + image_name)\n\n# delete the labels that do not have corresponding images\nfor label_name in label_names:\n    if label_name.replace('.txt', '.tif') not in image_names:\n        os.remove(label_dir + label_name)\n\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2542\nNumber of labels: 2542\n\n\n\n\n\n\nCode\ntotal_instances = 0\nclass_count = {}\n\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        total_instances += len(lines)\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\n\nclass_count = dict(sorted(class_count.items()))\n\nprint('Total instances:', total_instances)\nprint('\\nNumber of unique classes:', len(class_count))\nprint('\\nClass-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nTotal instances: 29625\n\nNumber of unique classes: 3\n\nClass-wise distribution:\n    Class 0: 29267\n    Class 1: 130\n    Class 2: 228\n\n\nSince, we are doing a detection task, I converted all labels of class 1 and 2 to class 0.\n\n\nCode\n# convert all classes to 0 in the labels\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    with open(label_path, 'w') as f:\n        for line in lines:\n            f.write('0 ' + ' '.join(line.split()[1:]) + '\\n')\n\n# updated class count\nclass_count = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\nprint('Updated class-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nUpdated class-wise distribution:\n    Class 0: 29625\n\n\n\n\nCode\n# Calculate number of images having a particular number of labels\nlabel_distribution = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        num_labels = len(lines)\n        label_distribution[num_labels] = label_distribution.get(num_labels, 0) + 1\n\nlabel_distribution = dict(sorted(label_distribution.items()))\nprint('Value counts of labels per image:')\nfor num_labels, count in label_distribution.items():\n    print(f'{count} images have {num_labels} labels.')\n\n\nValue counts of labels per image:\n81 images have 1 labels.\n167 images have 2 labels.\n221 images have 3 labels.\n218 images have 4 labels.\n217 images have 5 labels.\n189 images have 6 labels.\n170 images have 7 labels.\n184 images have 8 labels.\n169 images have 9 labels.\n121 images have 10 labels.\n97 images have 11 labels.\n84 images have 12 labels.\n69 images have 13 labels.\n49 images have 14 labels.\n46 images have 15 labels.\n41 images have 16 labels.\n36 images have 17 labels.\n25 images have 18 labels.\n29 images have 19 labels.\n14 images have 20 labels.\n4 images have 21 labels.\n1 images have 22 labels.\n4 images have 23 labels.\n2 images have 24 labels.\n4 images have 25 labels.\n3 images have 26 labels.\n5 images have 27 labels.\n5 images have 28 labels.\n15 images have 29 labels.\n20 images have 30 labels.\n8 images have 31 labels.\n7 images have 32 labels.\n13 images have 33 labels.\n19 images have 34 labels.\n10 images have 35 labels.\n6 images have 36 labels.\n17 images have 37 labels.\n13 images have 38 labels.\n6 images have 39 labels.\n9 images have 40 labels.\n10 images have 41 labels.\n12 images have 42 labels.\n11 images have 43 labels.\n4 images have 44 labels.\n2 images have 45 labels.\n5 images have 46 labels.\n9 images have 47 labels.\n3 images have 48 labels.\n5 images have 49 labels.\n6 images have 50 labels.\n9 images have 51 labels.\n16 images have 52 labels.\n4 images have 53 labels.\n6 images have 54 labels.\n1 images have 55 labels.\n1 images have 56 labels.\n3 images have 58 labels.\n2 images have 59 labels.\n2 images have 60 labels.\n1 images have 61 labels.\n6 images have 62 labels.\n3 images have 63 labels.\n1 images have 64 labels.\n3 images have 65 labels.\n4 images have 66 labels.\n1 images have 67 labels.\n1 images have 71 labels.\n1 images have 72 labels.\n1 images have 73 labels.\n5 images have 74 labels.\n1 images have 75 labels.\n2 images have 76 labels.\n2 images have 77 labels.\n1 images have 78 labels.\n\n\n\n\n\nWe can calculate the area of the solar panels (in square meters) as follows:\n\nDenormalize x-width and y-width:\n\nMultiply by the chip size:\n\nNative resolution (31 cm): \\(416 \\times 416\\)\nHD resolution (15.5 cm): \\(832 \\times 832\\)\n\n\nConvert to real-world meters:\n\nPixel size = 0.31 meters per pixel\nReal width & height in meters: \\[\n\\text{real\\_width} = x\\_width \\times 416 \\times 0.31\n\\] \\[\n\\text{real\\_height} = y\\_width \\times 416 \\times 0.31\n\\]\n\nCompute area: \\[\n\\text{area} = \\text{real\\_width} \\times \\text{real\\_height}\n\\]\n\n\n\nCode\nareas = []\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name, x_center, y_center, width, height = map(float, line.split())\n            # print(class_name, x_center, y_center, width, height)\n            real_width = width * image_size * meters_per_pixel\n            real_height = height * image_size * meters_per_pixel\n            area = real_width * real_height\n            areas.append(area)\n\nareas = np.array(areas)\nmean_area = np.mean(areas)\nstd_area = np.std(areas)\nprint(f'Mean area of solar panels: {mean_area:.2f} m^2')\nprint(f'Standard deviation of area of solar panels: {std_area:.2f} m^2')\n\nplt.figure(figsize=(8, 6))\nplt.hist(areas, bins=50, color='blue', edgecolor='black', alpha=0.7)\nplt.xlabel('Area (m^2)')\nplt.ylabel('Frequency')\nplt.title('Histogram of areas of solar panels')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\nMean area of solar panels: 191.52 m^2\nStandard deviation of area of solar panels: 630.70 m^2\n\n\n\n\n\n\n\n\n\nFrom the above histogram, we can observe the following:\n\nThe majority of the solar panels have areas concentrated around the lower end of the scale.\nThere are fewer instances of solar panels with larger areas.\nThe distribution appears to be right-skewed, indicating that most solar panels are relatively small in size, with a few larger ones.\n\n\n\n\n\n\n\nIntersection over Union (IoU) is a metric used to evaluate the accuracy of an object detector on a particular dataset. It measures the overlap between two bounding boxes: the predicted bounding box and the ground truth bounding box.\nThe IoU is calculated as follows:\n\nIntersection: The area of overlap between the predicted bounding box and the ground truth bounding box.\nUnion: The total area covered by both the predicted bounding box and the ground truth bounding box.\n\nThe IoU is then computed as the ratio of the intersection area to the union area:\n\\[\n\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n\\]\nThe IoU value ranges from 0 to 1, where:\n\n0 indicates no overlap between the bounding boxes.\n1 indicates a perfect overlap between the bounding boxes.\n\nUsage of Shapely Library:\nShapely’s Polygon class is used to represent bounding boxes as geometric shapes, defined by their corner coordinates. The intersection area between two polygons is computed using the .intersection() method, which finds the overlapping region of the two bounding boxes. The union area is determined using the .union() method, which combines both polygons into a single shape.\n\n\nCode\n# convert yolo format to x_min, y_min, x_max, y_max format\ndef yolo_to_xyxy(yolo_bbox, image_size):\n    if len(yolo_bbox) == 5:\n        class_id, x_center, y_center, width, height = yolo_bbox\n    else:\n        x_center, y_center, width, height = yolo_bbox\n    x_min = (x_center - width / 2) * image_size\n    y_min = (y_center - height / 2) * image_size\n    x_max = (x_center + width / 2) * image_size\n    y_max = (y_center + height / 2) * image_size\n    return x_min, y_min, x_max, y_max\n\n# calculate iou using shapely\ndef iou_shapely(yolo_bbox1, yolo_bbox2, image_size):\n    x_min1, y_min1, x_max1, y_max1 = yolo_to_xyxy(yolo_bbox1, image_size)\n    x_min2, y_min2, x_max2, y_max2 = yolo_to_xyxy(yolo_bbox2, image_size)\n\n    polygon1 = sg.Polygon([(x_min1, y_min1), (x_max1, y_min1), (x_max1, y_max1), (x_min1, y_max1)])\n    polygon2 = sg.Polygon([(x_min2, y_min2), (x_max2, y_min2), (x_max2, y_max2), (x_min2, y_max2)])\n\n    intersection_area = polygon1.intersection(polygon2).area\n    union_area = polygon1.union(polygon2).area\n\n    if union_area == 0.0:\n        iou = 0.0\n    else:\n        iou = intersection_area / union_area\n    return iou\n\n# calculate iou using supervision\ndef iou_supervision(yolo_bbox1, yolo_bbox2, image_size):\n    box1 = np.array([yolo_to_xyxy(yolo_bbox1, image_size)])\n    box2 = np.array([yolo_to_xyxy(yolo_bbox2, image_size)])\n    iou_matrix = sv.box_iou_batch(box1, box2)\n    return iou_matrix[0, 0]\n\n\n\n\nCode\n# Example usage on YOLO bounding boxes (class_id, x_center, y_center, width, height) \nyolo_bbox1 = [0, 0.1, 0.1, 0.6, 0.6]\nyolo_bbox2 = [0, 0.3, 0.3, 0.6, 0.6]\n\niou1 = iou_shapely(yolo_bbox1, yolo_bbox2, image_size)\niou2 = iou_supervision(yolo_bbox1, yolo_bbox2, image_size)\n\nprint(f'IoU computed using shapely: {iou1:.4f}')\nprint(f'IoU computed using supervision: {iou2:.4f}')\n\n\nIoU computed using shapely: 0.2857\nIoU computed using supervision: 0.2857\n\n\n\n\n\nAverage Precision (AP) is a metric used to evaluate the performance of object detection models. It summarizes the precision-recall curve into a single value, representing the average of precision values at different recall levels.\n\nPrecision: The ratio of true positive detections to the total number of positive detections (true positives + false positives).\n\\[\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\nRecall: The ratio of true positive detections to the total number of actual positives (true positives + false negatives).\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\n\nThe precision-recall curve is plotted with precision on the y-axis and recall on the x-axis. The Average Precision (AP) is calculated as the area under the precision-recall curve.\nThere are different methods to compute AP:\n\nPascal VOC 11-point interpolation: Precision is sampled at 11 recall levels (0.0, 0.1, …, 1.0), and the average of these precision values is taken.\nCOCO 101-point interpolation: Precision is sampled at 101 recall levels (0.0, 0.01, …, 1.0), and the average of these precision values is taken.\nArea under Precision-Recall Curve (PRC): The area under the precision-recall curve is computed using numerical integration methods.\n\n\n\nCode\nfrom sklearn.metrics import auc\n\ndef compute_ap(precisions, recalls, method=\"voc11\"):\n    # Pascal VOC 11-point method\n    if method == \"voc11\":\n        recall_points = np.linspace(0, 1, 11)\n        ap = 0.0\n        for r in recall_points:\n            ap += max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0\n        ap /= 11\n    # COCO 101-point method\n    elif method == \"coco101\":\n        recall_points = np.linspace(0, 1, 101)\n        ap = np.mean([max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0 for r in recall_points])\n    # AUC method\n    elif method == \"auc_pr\":\n        ap = auc(recalls, precisions)\n    else:\n        raise ValueError(\"Invalid AP computation method.\")\n    return ap\n\n\n# calculate precision and recall values\ndef compute_precision_recall(gt_boxes, pred_boxes, scores, image_size, iou_threshold=0.5):\n    all_tp = []\n    all_fp = []\n    num_gt_boxes = sum(len(gt) for gt in gt_boxes)\n    if num_gt_boxes == 0:\n        return np.array([]), np.array([])\n\n    for gt_boxes_image, pred_boxes_image, scores_image in zip(gt_boxes, pred_boxes, scores):\n        # Sort predicted boxes by confidence score in descending order\n        sorted_indices = np.argsort(scores_image)[::-1]\n        pred_boxes_image = [pred_boxes_image[idx] for idx in sorted_indices]\n        scores_image = [scores_image[idx] for idx in sorted_indices]\n\n        tp = np.zeros(len(pred_boxes_image))\n        fp = np.zeros(len(pred_boxes_image))\n        gt_matched = np.zeros(len(gt_boxes_image))\n\n        for j, pred_box in enumerate(pred_boxes_image):\n            if len(gt_boxes_image) == 0:\n                fp[j] = 1\n                continue\n            \n            ious = [iou_shapely(pred_box, gt_box, image_size) for gt_box in gt_boxes_image]\n            max_iou = max(ious) if ious else 0\n            max_iou_idx = np.argmax(ious) if ious else -1\n\n            if max_iou &gt;= iou_threshold and max_iou_idx != -1 and gt_matched[max_iou_idx] == 0:\n                tp[j] = 1\n                gt_matched[max_iou_idx] = 1\n            else:\n                fp[j] = 1\n\n        all_tp.extend(tp)\n        all_fp.extend(fp)\n\n    # Compute Precision-Recall\n    tp_cumsum = np.cumsum(all_tp)\n    fp_cumsum = np.cumsum(all_fp)\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)  # Avoid division by zero\n    recalls = tp_cumsum / num_gt_boxes\n\n    return precisions, recalls\n\n\n\n\n\n\n\nCode\n# generate random data\ndef generate_random_data(num_images, image_size, num_gt_boxes, num_pred_boxes, box_size):\n    images = []\n    gt_boxes = []\n    pred_boxes = []\n    scores = []\n\n    for _ in range(num_images):\n        image = np.random.rand(image_size, image_size)\n        images.append(image)\n\n        gt_boxes_image = []\n        for _ in range(num_gt_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            gt_boxes_image.append([x_min, y_min, x_max, y_max])\n        gt_boxes.append(gt_boxes_image)\n\n        pred_boxes_image = []\n        scores_image = []\n        for _ in range(num_pred_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            pred_boxes_image.append([x_min, y_min, x_max, y_max])\n            scores_image.append(np.random.uniform(0.5, 1.0))\n        pred_boxes.append(pred_boxes_image)\n        scores.append(scores_image)\n\n    return images, gt_boxes, pred_boxes, scores\n\n\n\n\nCode\nrandom_num_images = 10\nrandom_image_size = 100\nrandom_num_gt_boxes = 10\nrandom_num_pred_boxes = 10\nrandom_box_size = 20\nrandom_images, random_gt_boxes, random_pred_boxes, random_scores = generate_random_data(random_num_images, random_image_size, random_num_gt_boxes, random_num_pred_boxes, random_box_size)\n\n\n\n\nCode\nprecisions, recalls = compute_precision_recall(random_gt_boxes, random_pred_boxes, random_scores, random_image_size, iou_threshold=0.5)\n\n# plot precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recalls, precisions, color='blue', linestyle='-', linewidth=2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute AP50\nap50_voc11 = compute_ap(precisions, recalls, method=\"voc11\")\nap50_coco101 = compute_ap(precisions, recalls, method=\"coco101\")\nap50_auc_pr = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f'AP50 using VOC11 method: {ap50_voc11:.4f}')\nprint(f'AP50 using COCO101 method: {ap50_coco101:.4f}')\nprint(f'AP50 using AUC-PR method: {ap50_auc_pr:.4f}')\n\n\nAP50 using VOC11 method: 0.2685\nAP50 using COCO101 method: 0.2571\nAP50 using AUC-PR method: 0.2188\n\n\n\n\n\n\n\n\nWe will split the data into training and testing sets using an 80-20 split. We will further split the training data into training and validation sets using a 90-10 split. This will allow us to train the model on the training set and tune the hyperparameters on the validation set. The testing set will be used to evaluate the model’s performance on unseen data.\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\ndef split_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1):\n    os.makedirs(save_dir, exist_ok=True)\n    for split in [\"train\", \"val\", \"test\"]:\n        os.makedirs(os.path.join(save_dir, split, \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(save_dir, split, \"labels\"), exist_ok=True)\n    \n    image_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\n    label_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n    num_images = len(image_names)\n\n    # shuffle the data\n    indices = np.arange(num_images)\n    np.random.shuffle(indices)\n\n    train_size = int(train_ratio * num_images)\n    val_size = int(val_ratio * train_size)\n    test_size = num_images - train_size\n\n    train_indices = indices[:train_size]\n    val_indices = train_indices[-val_size:]\n    train_indices = train_indices[:-val_size]\n    test_indices = indices[train_size:]\n\n    def copy_data(indices, split):\n        for idx in indices:\n            image_src = os.path.join(image_dir, image_names[idx])\n            label_src = os.path.join(label_dir, label_names[idx])\n\n            image_dst = os.path.join(save_dir, split, \"images\", image_names[idx])\n            label_dst = os.path.join(save_dir, split, \"labels\", label_names[idx])\n\n            shutil.copy(image_src, image_dst)\n            shutil.copy(label_src, label_dst)\n\n    copy_data(train_indices, \"train\")\n    copy_data(val_indices, \"val\")\n    copy_data(test_indices, \"test\")\n\n    print(f\"Data split and saved in '{save_dir}/' successfully!\")\n    print(f\"Train:      {len(train_indices)} images\")\n    print(f\"Validation: {len(val_indices)} images\")\n    print(f\"Test:       {len(test_indices)} images\")\n\n\n\nsplit_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1)\n\n\nData split and saved in 'split_data/' successfully!\nTrain:      1830 images\nValidation: 203 images\nTest:       509 images\n\n\n\n\nCode\n# Create YAML content with absolute paths\nbase_path = os.path.abspath('./split_data')\nyaml_content = f\"\"\"path: {base_path}\ntrain: {os.path.join(base_path, 'train', 'images')}\nval: {os.path.join(base_path, 'val', 'images')}\ntest: {os.path.join(base_path, 'test', 'images')}\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\"\"\"\ndata_yml_path = os.path.join(base_path, 'data.yaml')\nwith open(data_yml_path, 'w') as f:\n    f.write(yaml_content)\nprint(f\"data.yaml created at:\\n{data_yml_path}\")\ndel base_path\n\n\ndata.yaml created at:\nc:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\data.yaml\n\n\n\n\nCode\nprint(yaml_content)\n\n\npath: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\ntrain: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\train\\images\nval: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\val\\images\ntest: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\n\n\n\n\n\n\n\nCode\nmodel = YOLO(\"yolo12x.pt\")\n\nresults = model.train(\n    data=\"split_data/data.yaml\",\n    epochs=100,\n    imgsz=416,\n    batch=100,\n    device=device,\n    project=\"models\",\n    name=\"yolo12x\",\n)\n\n\n\n\nCode\n# load the best model\nmodel = YOLO(\"models/yolo12x/weights/best.pt\")\n\n\n\n\nCode\ndf = pd.read_csv('models/yolo12x/results.csv')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# First subplot: Training loss\nax1.plot(df['epoch'], df['train/box_loss'], label='Box Loss')\nax1.plot(df['epoch'], df['train/cls_loss'], label='Cls Loss')\nax1.plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss')\ndf['train/total_loss'] = df['train/box_loss'] + df['train/cls_loss'] + df['train/dfl_loss']\nax1.plot(df['epoch'], df['train/total_loss'], label='Total Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training Loss')\nax1.legend()\nax1.grid(True)\n\n# Second subplot: Validation loss\nax2.plot(df['epoch'], df['val/box_loss'], label='Box Loss')\nax2.plot(df['epoch'], df['val/cls_loss'], label='Cls Loss')\nax2.plot(df['epoch'], df['val/dfl_loss'], label='DFL Loss')\ndf['val/total_loss'] = df['val/box_loss'] + df['val/cls_loss'] + df['val/dfl_loss']\nax2.plot(df['epoch'], df['val/total_loss'], label='Total Loss')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss')\nax2.set_title('Validation Loss')\nax2.legend()\nax2.set_ylim(ax1.get_ylim())\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load the test data\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\n# Randomly select 3-4 images\nnum_images = 4\nrandom_images = np.random.choice(test_images, num_images, replace=False)\n\nfig, axes = plt.subplots(num_images, 2, figsize=(15, 5 * num_images))\n\nfor i, image_name in enumerate(random_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Predict using the trained model\n    results = model(image_path)[0]  # Get the first result object\n\n    # Load the image\n    image = plt.imread(image_path)\n\n    # Plot the ground truth labels (green)\n    with open(label_path, 'r') as f:\n        gt_labels = f.readlines()\n    axes[i, 0].imshow(image)\n    axes[i, 0].axis('off')\n    axes[i, 0].set_title(f\"Ground Truth: {image_name}\")\n    for gt_label in gt_labels:\n        class_id, x_center, y_center, width, height = map(float, gt_label.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='green', facecolor='none')\n        axes[i, 0].add_patch(rect)\n        axes[i, 0].text(x_min, y_min - 5, 'Ground Truth', color='green', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\n    # Plot the predicted labels (red)\n    axes[i, 1].imshow(image)\n    axes[i, 1].axis('off')\n    axes[i, 1].set_title(f\"Predictions: {image_name}\")\n    boxes = results.boxes\n    for box in boxes:\n        x_min, y_min, x_max, y_max = box.xyxy[0].cpu().numpy()\n        conf = float(box.conf[0])\n        cls = int(box.cls[0])\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='blue', facecolor='none')\n        axes[i, 1].add_patch(rect)\n        axes[i, 1].text(x_min, y_max + 15, f'Pred: {conf:.2f}', color='blue', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\n# save figure\nfig.savefig('predictions.png')\n\n\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3870_y0_11610_dxdy_416.tif: 416x416 19 solar_panels, 124.0ms\nSpeed: 2.3ms preprocess, 124.0ms inference, 4.5ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3061_y0_8047_dxdy_416.tif: 416x416 5 solar_panels, 84.0ms\nSpeed: 2.5ms preprocess, 84.0ms inference, 4.9ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_7463_y0_9890_dxdy_416.tif: 416x416 8 solar_panels, 91.4ms\nSpeed: 1.7ms preprocess, 91.4ms inference, 4.3ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_8067_y0_10432_dxdy_416.tif: 416x416 2 solar_panels, 69.2ms\nSpeed: 1.6ms preprocess, 69.2ms inference, 4.6ms postprocess per image at shape (1, 3, 416, 416)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef load_detections(image_path):\n    label_path = image_path.replace('images', 'labels').replace('.tif', '.txt')\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    \n    xyxy_list = []\n    class_ids = []\n    scores = []\n\n    for line in lines:\n        class_id, x_center, y_center, width, height = map(float, line.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        xyxy_list.append([x_min, y_min, x_max, y_max])\n        class_ids.append(class_id)\n        scores.append(1.0)\n\n    detections = sv.Detections(\n        xyxy=np.array(xyxy_list),\n        class_id=np.array(class_ids),\n        confidence=np.array(scores),\n        metadata={\"image_name\": os.path.basename(image_path)}\n    )\n\n    return detections\n\n\n\n\nCode\n# load the ground truth and predictions\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\ntargets = []\npredictions = []\n\nfor i, image_name in enumerate(test_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Load the ground truth and predictions\n    target_sv_detection = load_detections(image_path)\n    # print(target_sv_detection.xyxy)\n\n    results = model(image_path, verbose=False)[0] \n    pred_sv_detection = sv.Detections.from_ultralytics(results)\n    # print(pred_sv_detection.xyxy)\n\n    targets.append(target_sv_detection)\n    predictions.append(pred_sv_detection)\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\nimport pickle\n\n# Save the detections\nwith open(\"detections.pkl\", \"wb\") as f:\n    pickle.dump({\"targets\": targets, \"predictions\": predictions}, f)\n\n# Load the detections\nwith open(\"detections.pkl\", \"rb\") as f:\n    data = pickle.load(f)\ntargets = data[\"targets\"]\npredictions = data[\"predictions\"]\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\n# Compute mAP50 using supervision\nmetrics = sv.metrics.MeanAveragePrecision()\nresults = metrics.update(targets, predictions).compute()\nmAP50_supervision = results.map50\nprint(f\"mAP50 using supervision: {mAP50_supervision:.4f}\")\n\n\nmAP50 using supervision: 0.9018\n\n\n\n\nCode\n# Compute mAP50 using my implementation\nall_gt_boxes, all_pred_boxes, all_scores = [], [], []\n\nfor target, prediction in zip(targets, predictions):\n    all_gt_boxes.append(target.xyxy)\n    all_pred_boxes.append(prediction.xyxy)\n    all_scores.append(prediction.confidence)\n\nprecisions, recalls = compute_precision_recall(all_gt_boxes, all_pred_boxes, all_scores, image_size, iou_threshold=0.5)\nmAP50_mine = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f\"mAP50 using my implementation: {mAP50_mine:.4f}\")\n\n\nmAP50 using my implementation: 0.8930\n\n\nTODO:\nCreate a table of Precision, Recall and F1-scores where rows are IoU thresholds [0.1, 0.3, 0.5, 0.7, 0.9] and columns are confidence thresholds [0.1, 0.3, 0.5, 0.7, 0.9]\n(Hint use supervision.metrics.ConfusionMatrix to get the confusion matrix and get TP, FP and FN from it to compute the P, R and F-1)"
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html#data-exploration-and-understanding",
    "href": "posts/solar_panel_detector/solar_panel_detector.html#data-exploration-and-understanding",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "We will use the Solar Panel Object Labels dataset from Figshare. This dataset contains high-resolution aerial images with labeled solar panels. We will use the 31 cm native resolution images of sizes 416x416 pixels for our analysis.\nThe dataset files follow a specific naming structure: solarpanels_native_&lt;image_number&gt;__x0_&lt;x_coordinate&gt;_y0_&lt;y_coordinate&gt;_dxdy_&lt;size&gt;.\nFor example, in the file solarpanels_native_1__x0_0_y0_6845_dxdy_416.txt:\n\nsolarpanels_native: Indicates that the file contains solar panel data in native resolution.\n&lt;image_number&gt;: The number of the image in the dataset.\nx0_&lt;x_coordinate&gt;: The x-coordinate of the top-left corner of the image.\ny0_&lt;y_coordinate&gt;: The y-coordinate of the top-left corner of the image.\ndxdy_&lt;size&gt;: The size of the image in pixels (both width and height).\n\nEach line in the file represents a detected solar panel with the following format: category x_center y_center width height, where:\n\ncategory: The category label (0 for solar panels).\nx_center: The x-coordinate of the center of the bounding box (normalized).\ny_center: The y-coordinate of the center of the bounding box (normalized).\nwidth: The width of the bounding box (normalized).\nheight: The height of the bounding box (normalized).\n\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2553\nNumber of labels: 2542\n\n\nThe number of images and the number of labels is not the same. So, we can remove the images that do not have corresponding labels and remove the labels that do not have corresponding images.\n\n\nCode\n# delete the images that do not have corresponding labels\nfor image_name in image_names:\n    if image_name.replace('.tif', '.txt') not in label_names:\n        os.remove(image_dir + image_name)\n\n# delete the labels that do not have corresponding images\nfor label_name in label_names:\n    if label_name.replace('.txt', '.tif') not in image_names:\n        os.remove(label_dir + label_name)\n\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2542\nNumber of labels: 2542\n\n\n\n\n\n\nCode\ntotal_instances = 0\nclass_count = {}\n\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        total_instances += len(lines)\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\n\nclass_count = dict(sorted(class_count.items()))\n\nprint('Total instances:', total_instances)\nprint('\\nNumber of unique classes:', len(class_count))\nprint('\\nClass-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nTotal instances: 29625\n\nNumber of unique classes: 3\n\nClass-wise distribution:\n    Class 0: 29267\n    Class 1: 130\n    Class 2: 228\n\n\nSince, we are doing a detection task, I converted all labels of class 1 and 2 to class 0.\n\n\nCode\n# convert all classes to 0 in the labels\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    with open(label_path, 'w') as f:\n        for line in lines:\n            f.write('0 ' + ' '.join(line.split()[1:]) + '\\n')\n\n# updated class count\nclass_count = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\nprint('Updated class-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nUpdated class-wise distribution:\n    Class 0: 29625\n\n\n\n\nCode\n# Calculate number of images having a particular number of labels\nlabel_distribution = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        num_labels = len(lines)\n        label_distribution[num_labels] = label_distribution.get(num_labels, 0) + 1\n\nlabel_distribution = dict(sorted(label_distribution.items()))\nprint('Value counts of labels per image:')\nfor num_labels, count in label_distribution.items():\n    print(f'{count} images have {num_labels} labels.')\n\n\nValue counts of labels per image:\n81 images have 1 labels.\n167 images have 2 labels.\n221 images have 3 labels.\n218 images have 4 labels.\n217 images have 5 labels.\n189 images have 6 labels.\n170 images have 7 labels.\n184 images have 8 labels.\n169 images have 9 labels.\n121 images have 10 labels.\n97 images have 11 labels.\n84 images have 12 labels.\n69 images have 13 labels.\n49 images have 14 labels.\n46 images have 15 labels.\n41 images have 16 labels.\n36 images have 17 labels.\n25 images have 18 labels.\n29 images have 19 labels.\n14 images have 20 labels.\n4 images have 21 labels.\n1 images have 22 labels.\n4 images have 23 labels.\n2 images have 24 labels.\n4 images have 25 labels.\n3 images have 26 labels.\n5 images have 27 labels.\n5 images have 28 labels.\n15 images have 29 labels.\n20 images have 30 labels.\n8 images have 31 labels.\n7 images have 32 labels.\n13 images have 33 labels.\n19 images have 34 labels.\n10 images have 35 labels.\n6 images have 36 labels.\n17 images have 37 labels.\n13 images have 38 labels.\n6 images have 39 labels.\n9 images have 40 labels.\n10 images have 41 labels.\n12 images have 42 labels.\n11 images have 43 labels.\n4 images have 44 labels.\n2 images have 45 labels.\n5 images have 46 labels.\n9 images have 47 labels.\n3 images have 48 labels.\n5 images have 49 labels.\n6 images have 50 labels.\n9 images have 51 labels.\n16 images have 52 labels.\n4 images have 53 labels.\n6 images have 54 labels.\n1 images have 55 labels.\n1 images have 56 labels.\n3 images have 58 labels.\n2 images have 59 labels.\n2 images have 60 labels.\n1 images have 61 labels.\n6 images have 62 labels.\n3 images have 63 labels.\n1 images have 64 labels.\n3 images have 65 labels.\n4 images have 66 labels.\n1 images have 67 labels.\n1 images have 71 labels.\n1 images have 72 labels.\n1 images have 73 labels.\n5 images have 74 labels.\n1 images have 75 labels.\n2 images have 76 labels.\n2 images have 77 labels.\n1 images have 78 labels.\n\n\n\n\n\nWe can calculate the area of the solar panels (in square meters) as follows:\n\nDenormalize x-width and y-width:\n\nMultiply by the chip size:\n\nNative resolution (31 cm): \\(416 \\times 416\\)\nHD resolution (15.5 cm): \\(832 \\times 832\\)\n\n\nConvert to real-world meters:\n\nPixel size = 0.31 meters per pixel\nReal width & height in meters: \\[\n\\text{real\\_width} = x\\_width \\times 416 \\times 0.31\n\\] \\[\n\\text{real\\_height} = y\\_width \\times 416 \\times 0.31\n\\]\n\nCompute area: \\[\n\\text{area} = \\text{real\\_width} \\times \\text{real\\_height}\n\\]\n\n\n\nCode\nareas = []\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name, x_center, y_center, width, height = map(float, line.split())\n            # print(class_name, x_center, y_center, width, height)\n            real_width = width * image_size * meters_per_pixel\n            real_height = height * image_size * meters_per_pixel\n            area = real_width * real_height\n            areas.append(area)\n\nareas = np.array(areas)\nmean_area = np.mean(areas)\nstd_area = np.std(areas)\nprint(f'Mean area of solar panels: {mean_area:.2f} m^2')\nprint(f'Standard deviation of area of solar panels: {std_area:.2f} m^2')\n\nplt.figure(figsize=(8, 6))\nplt.hist(areas, bins=50, color='blue', edgecolor='black', alpha=0.7)\nplt.xlabel('Area (m^2)')\nplt.ylabel('Frequency')\nplt.title('Histogram of areas of solar panels')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\nMean area of solar panels: 191.52 m^2\nStandard deviation of area of solar panels: 630.70 m^2\n\n\n\n\n\n\n\n\n\nFrom the above histogram, we can observe the following:\n\nThe majority of the solar panels have areas concentrated around the lower end of the scale.\nThere are fewer instances of solar panels with larger areas.\nThe distribution appears to be right-skewed, indicating that most solar panels are relatively small in size, with a few larger ones."
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html#implementing-the-fundamental-functions",
    "href": "posts/solar_panel_detector/solar_panel_detector.html#implementing-the-fundamental-functions",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "Intersection over Union (IoU) is a metric used to evaluate the accuracy of an object detector on a particular dataset. It measures the overlap between two bounding boxes: the predicted bounding box and the ground truth bounding box.\nThe IoU is calculated as follows:\n\nIntersection: The area of overlap between the predicted bounding box and the ground truth bounding box.\nUnion: The total area covered by both the predicted bounding box and the ground truth bounding box.\n\nThe IoU is then computed as the ratio of the intersection area to the union area:\n\\[\n\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n\\]\nThe IoU value ranges from 0 to 1, where:\n\n0 indicates no overlap between the bounding boxes.\n1 indicates a perfect overlap between the bounding boxes.\n\nUsage of Shapely Library:\nShapely’s Polygon class is used to represent bounding boxes as geometric shapes, defined by their corner coordinates. The intersection area between two polygons is computed using the .intersection() method, which finds the overlapping region of the two bounding boxes. The union area is determined using the .union() method, which combines both polygons into a single shape.\n\n\nCode\n# convert yolo format to x_min, y_min, x_max, y_max format\ndef yolo_to_xyxy(yolo_bbox, image_size):\n    if len(yolo_bbox) == 5:\n        class_id, x_center, y_center, width, height = yolo_bbox\n    else:\n        x_center, y_center, width, height = yolo_bbox\n    x_min = (x_center - width / 2) * image_size\n    y_min = (y_center - height / 2) * image_size\n    x_max = (x_center + width / 2) * image_size\n    y_max = (y_center + height / 2) * image_size\n    return x_min, y_min, x_max, y_max\n\n# calculate iou using shapely\ndef iou_shapely(yolo_bbox1, yolo_bbox2, image_size):\n    x_min1, y_min1, x_max1, y_max1 = yolo_to_xyxy(yolo_bbox1, image_size)\n    x_min2, y_min2, x_max2, y_max2 = yolo_to_xyxy(yolo_bbox2, image_size)\n\n    polygon1 = sg.Polygon([(x_min1, y_min1), (x_max1, y_min1), (x_max1, y_max1), (x_min1, y_max1)])\n    polygon2 = sg.Polygon([(x_min2, y_min2), (x_max2, y_min2), (x_max2, y_max2), (x_min2, y_max2)])\n\n    intersection_area = polygon1.intersection(polygon2).area\n    union_area = polygon1.union(polygon2).area\n\n    if union_area == 0.0:\n        iou = 0.0\n    else:\n        iou = intersection_area / union_area\n    return iou\n\n# calculate iou using supervision\ndef iou_supervision(yolo_bbox1, yolo_bbox2, image_size):\n    box1 = np.array([yolo_to_xyxy(yolo_bbox1, image_size)])\n    box2 = np.array([yolo_to_xyxy(yolo_bbox2, image_size)])\n    iou_matrix = sv.box_iou_batch(box1, box2)\n    return iou_matrix[0, 0]\n\n\n\n\nCode\n# Example usage on YOLO bounding boxes (class_id, x_center, y_center, width, height) \nyolo_bbox1 = [0, 0.1, 0.1, 0.6, 0.6]\nyolo_bbox2 = [0, 0.3, 0.3, 0.6, 0.6]\n\niou1 = iou_shapely(yolo_bbox1, yolo_bbox2, image_size)\niou2 = iou_supervision(yolo_bbox1, yolo_bbox2, image_size)\n\nprint(f'IoU computed using shapely: {iou1:.4f}')\nprint(f'IoU computed using supervision: {iou2:.4f}')\n\n\nIoU computed using shapely: 0.2857\nIoU computed using supervision: 0.2857\n\n\n\n\n\nAverage Precision (AP) is a metric used to evaluate the performance of object detection models. It summarizes the precision-recall curve into a single value, representing the average of precision values at different recall levels.\n\nPrecision: The ratio of true positive detections to the total number of positive detections (true positives + false positives).\n\\[\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\nRecall: The ratio of true positive detections to the total number of actual positives (true positives + false negatives).\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\n\nThe precision-recall curve is plotted with precision on the y-axis and recall on the x-axis. The Average Precision (AP) is calculated as the area under the precision-recall curve.\nThere are different methods to compute AP:\n\nPascal VOC 11-point interpolation: Precision is sampled at 11 recall levels (0.0, 0.1, …, 1.0), and the average of these precision values is taken.\nCOCO 101-point interpolation: Precision is sampled at 101 recall levels (0.0, 0.01, …, 1.0), and the average of these precision values is taken.\nArea under Precision-Recall Curve (PRC): The area under the precision-recall curve is computed using numerical integration methods.\n\n\n\nCode\nfrom sklearn.metrics import auc\n\ndef compute_ap(precisions, recalls, method=\"voc11\"):\n    # Pascal VOC 11-point method\n    if method == \"voc11\":\n        recall_points = np.linspace(0, 1, 11)\n        ap = 0.0\n        for r in recall_points:\n            ap += max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0\n        ap /= 11\n    # COCO 101-point method\n    elif method == \"coco101\":\n        recall_points = np.linspace(0, 1, 101)\n        ap = np.mean([max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0 for r in recall_points])\n    # AUC method\n    elif method == \"auc_pr\":\n        ap = auc(recalls, precisions)\n    else:\n        raise ValueError(\"Invalid AP computation method.\")\n    return ap\n\n\n# calculate precision and recall values\ndef compute_precision_recall(gt_boxes, pred_boxes, scores, image_size, iou_threshold=0.5):\n    all_tp = []\n    all_fp = []\n    num_gt_boxes = sum(len(gt) for gt in gt_boxes)\n    if num_gt_boxes == 0:\n        return np.array([]), np.array([])\n\n    for gt_boxes_image, pred_boxes_image, scores_image in zip(gt_boxes, pred_boxes, scores):\n        # Sort predicted boxes by confidence score in descending order\n        sorted_indices = np.argsort(scores_image)[::-1]\n        pred_boxes_image = [pred_boxes_image[idx] for idx in sorted_indices]\n        scores_image = [scores_image[idx] for idx in sorted_indices]\n\n        tp = np.zeros(len(pred_boxes_image))\n        fp = np.zeros(len(pred_boxes_image))\n        gt_matched = np.zeros(len(gt_boxes_image))\n\n        for j, pred_box in enumerate(pred_boxes_image):\n            if len(gt_boxes_image) == 0:\n                fp[j] = 1\n                continue\n            \n            ious = [iou_shapely(pred_box, gt_box, image_size) for gt_box in gt_boxes_image]\n            max_iou = max(ious) if ious else 0\n            max_iou_idx = np.argmax(ious) if ious else -1\n\n            if max_iou &gt;= iou_threshold and max_iou_idx != -1 and gt_matched[max_iou_idx] == 0:\n                tp[j] = 1\n                gt_matched[max_iou_idx] = 1\n            else:\n                fp[j] = 1\n\n        all_tp.extend(tp)\n        all_fp.extend(fp)\n\n    # Compute Precision-Recall\n    tp_cumsum = np.cumsum(all_tp)\n    fp_cumsum = np.cumsum(all_fp)\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)  # Avoid division by zero\n    recalls = tp_cumsum / num_gt_boxes\n\n    return precisions, recalls\n\n\n\n\n\n\n\nCode\n# generate random data\ndef generate_random_data(num_images, image_size, num_gt_boxes, num_pred_boxes, box_size):\n    images = []\n    gt_boxes = []\n    pred_boxes = []\n    scores = []\n\n    for _ in range(num_images):\n        image = np.random.rand(image_size, image_size)\n        images.append(image)\n\n        gt_boxes_image = []\n        for _ in range(num_gt_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            gt_boxes_image.append([x_min, y_min, x_max, y_max])\n        gt_boxes.append(gt_boxes_image)\n\n        pred_boxes_image = []\n        scores_image = []\n        for _ in range(num_pred_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            pred_boxes_image.append([x_min, y_min, x_max, y_max])\n            scores_image.append(np.random.uniform(0.5, 1.0))\n        pred_boxes.append(pred_boxes_image)\n        scores.append(scores_image)\n\n    return images, gt_boxes, pred_boxes, scores\n\n\n\n\nCode\nrandom_num_images = 10\nrandom_image_size = 100\nrandom_num_gt_boxes = 10\nrandom_num_pred_boxes = 10\nrandom_box_size = 20\nrandom_images, random_gt_boxes, random_pred_boxes, random_scores = generate_random_data(random_num_images, random_image_size, random_num_gt_boxes, random_num_pred_boxes, random_box_size)\n\n\n\n\nCode\nprecisions, recalls = compute_precision_recall(random_gt_boxes, random_pred_boxes, random_scores, random_image_size, iou_threshold=0.5)\n\n# plot precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recalls, precisions, color='blue', linestyle='-', linewidth=2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute AP50\nap50_voc11 = compute_ap(precisions, recalls, method=\"voc11\")\nap50_coco101 = compute_ap(precisions, recalls, method=\"coco101\")\nap50_auc_pr = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f'AP50 using VOC11 method: {ap50_voc11:.4f}')\nprint(f'AP50 using COCO101 method: {ap50_coco101:.4f}')\nprint(f'AP50 using AUC-PR method: {ap50_auc_pr:.4f}')\n\n\nAP50 using VOC11 method: 0.2685\nAP50 using COCO101 method: 0.2571\nAP50 using AUC-PR method: 0.2188"
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html#model-building-and-evaluation",
    "href": "posts/solar_panel_detector/solar_panel_detector.html#model-building-and-evaluation",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "We will split the data into training and testing sets using an 80-20 split. We will further split the training data into training and validation sets using a 90-10 split. This will allow us to train the model on the training set and tune the hyperparameters on the validation set. The testing set will be used to evaluate the model’s performance on unseen data.\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\ndef split_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1):\n    os.makedirs(save_dir, exist_ok=True)\n    for split in [\"train\", \"val\", \"test\"]:\n        os.makedirs(os.path.join(save_dir, split, \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(save_dir, split, \"labels\"), exist_ok=True)\n    \n    image_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\n    label_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n    num_images = len(image_names)\n\n    # shuffle the data\n    indices = np.arange(num_images)\n    np.random.shuffle(indices)\n\n    train_size = int(train_ratio * num_images)\n    val_size = int(val_ratio * train_size)\n    test_size = num_images - train_size\n\n    train_indices = indices[:train_size]\n    val_indices = train_indices[-val_size:]\n    train_indices = train_indices[:-val_size]\n    test_indices = indices[train_size:]\n\n    def copy_data(indices, split):\n        for idx in indices:\n            image_src = os.path.join(image_dir, image_names[idx])\n            label_src = os.path.join(label_dir, label_names[idx])\n\n            image_dst = os.path.join(save_dir, split, \"images\", image_names[idx])\n            label_dst = os.path.join(save_dir, split, \"labels\", label_names[idx])\n\n            shutil.copy(image_src, image_dst)\n            shutil.copy(label_src, label_dst)\n\n    copy_data(train_indices, \"train\")\n    copy_data(val_indices, \"val\")\n    copy_data(test_indices, \"test\")\n\n    print(f\"Data split and saved in '{save_dir}/' successfully!\")\n    print(f\"Train:      {len(train_indices)} images\")\n    print(f\"Validation: {len(val_indices)} images\")\n    print(f\"Test:       {len(test_indices)} images\")\n\n\n\nsplit_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1)\n\n\nData split and saved in 'split_data/' successfully!\nTrain:      1830 images\nValidation: 203 images\nTest:       509 images\n\n\n\n\nCode\n# Create YAML content with absolute paths\nbase_path = os.path.abspath('./split_data')\nyaml_content = f\"\"\"path: {base_path}\ntrain: {os.path.join(base_path, 'train', 'images')}\nval: {os.path.join(base_path, 'val', 'images')}\ntest: {os.path.join(base_path, 'test', 'images')}\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\"\"\"\ndata_yml_path = os.path.join(base_path, 'data.yaml')\nwith open(data_yml_path, 'w') as f:\n    f.write(yaml_content)\nprint(f\"data.yaml created at:\\n{data_yml_path}\")\ndel base_path\n\n\ndata.yaml created at:\nc:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\data.yaml\n\n\n\n\nCode\nprint(yaml_content)\n\n\npath: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\ntrain: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\train\\images\nval: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\val\\images\ntest: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\n\n\n\n\n\n\n\nCode\nmodel = YOLO(\"yolo12x.pt\")\n\nresults = model.train(\n    data=\"split_data/data.yaml\",\n    epochs=100,\n    imgsz=416,\n    batch=100,\n    device=device,\n    project=\"models\",\n    name=\"yolo12x\",\n)\n\n\n\n\nCode\n# load the best model\nmodel = YOLO(\"models/yolo12x/weights/best.pt\")\n\n\n\n\nCode\ndf = pd.read_csv('models/yolo12x/results.csv')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# First subplot: Training loss\nax1.plot(df['epoch'], df['train/box_loss'], label='Box Loss')\nax1.plot(df['epoch'], df['train/cls_loss'], label='Cls Loss')\nax1.plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss')\ndf['train/total_loss'] = df['train/box_loss'] + df['train/cls_loss'] + df['train/dfl_loss']\nax1.plot(df['epoch'], df['train/total_loss'], label='Total Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training Loss')\nax1.legend()\nax1.grid(True)\n\n# Second subplot: Validation loss\nax2.plot(df['epoch'], df['val/box_loss'], label='Box Loss')\nax2.plot(df['epoch'], df['val/cls_loss'], label='Cls Loss')\nax2.plot(df['epoch'], df['val/dfl_loss'], label='DFL Loss')\ndf['val/total_loss'] = df['val/box_loss'] + df['val/cls_loss'] + df['val/dfl_loss']\nax2.plot(df['epoch'], df['val/total_loss'], label='Total Loss')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss')\nax2.set_title('Validation Loss')\nax2.legend()\nax2.set_ylim(ax1.get_ylim())\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load the test data\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\n# Randomly select 3-4 images\nnum_images = 4\nrandom_images = np.random.choice(test_images, num_images, replace=False)\n\nfig, axes = plt.subplots(num_images, 2, figsize=(15, 5 * num_images))\n\nfor i, image_name in enumerate(random_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Predict using the trained model\n    results = model(image_path)[0]  # Get the first result object\n\n    # Load the image\n    image = plt.imread(image_path)\n\n    # Plot the ground truth labels (green)\n    with open(label_path, 'r') as f:\n        gt_labels = f.readlines()\n    axes[i, 0].imshow(image)\n    axes[i, 0].axis('off')\n    axes[i, 0].set_title(f\"Ground Truth: {image_name}\")\n    for gt_label in gt_labels:\n        class_id, x_center, y_center, width, height = map(float, gt_label.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='green', facecolor='none')\n        axes[i, 0].add_patch(rect)\n        axes[i, 0].text(x_min, y_min - 5, 'Ground Truth', color='green', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\n    # Plot the predicted labels (red)\n    axes[i, 1].imshow(image)\n    axes[i, 1].axis('off')\n    axes[i, 1].set_title(f\"Predictions: {image_name}\")\n    boxes = results.boxes\n    for box in boxes:\n        x_min, y_min, x_max, y_max = box.xyxy[0].cpu().numpy()\n        conf = float(box.conf[0])\n        cls = int(box.cls[0])\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='blue', facecolor='none')\n        axes[i, 1].add_patch(rect)\n        axes[i, 1].text(x_min, y_max + 15, f'Pred: {conf:.2f}', color='blue', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\n# save figure\nfig.savefig('predictions.png')\n\n\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3870_y0_11610_dxdy_416.tif: 416x416 19 solar_panels, 124.0ms\nSpeed: 2.3ms preprocess, 124.0ms inference, 4.5ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3061_y0_8047_dxdy_416.tif: 416x416 5 solar_panels, 84.0ms\nSpeed: 2.5ms preprocess, 84.0ms inference, 4.9ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_7463_y0_9890_dxdy_416.tif: 416x416 8 solar_panels, 91.4ms\nSpeed: 1.7ms preprocess, 91.4ms inference, 4.3ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_8067_y0_10432_dxdy_416.tif: 416x416 2 solar_panels, 69.2ms\nSpeed: 1.6ms preprocess, 69.2ms inference, 4.6ms postprocess per image at shape (1, 3, 416, 416)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef load_detections(image_path):\n    label_path = image_path.replace('images', 'labels').replace('.tif', '.txt')\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    \n    xyxy_list = []\n    class_ids = []\n    scores = []\n\n    for line in lines:\n        class_id, x_center, y_center, width, height = map(float, line.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        xyxy_list.append([x_min, y_min, x_max, y_max])\n        class_ids.append(class_id)\n        scores.append(1.0)\n\n    detections = sv.Detections(\n        xyxy=np.array(xyxy_list),\n        class_id=np.array(class_ids),\n        confidence=np.array(scores),\n        metadata={\"image_name\": os.path.basename(image_path)}\n    )\n\n    return detections\n\n\n\n\nCode\n# load the ground truth and predictions\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\ntargets = []\npredictions = []\n\nfor i, image_name in enumerate(test_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Load the ground truth and predictions\n    target_sv_detection = load_detections(image_path)\n    # print(target_sv_detection.xyxy)\n\n    results = model(image_path, verbose=False)[0] \n    pred_sv_detection = sv.Detections.from_ultralytics(results)\n    # print(pred_sv_detection.xyxy)\n\n    targets.append(target_sv_detection)\n    predictions.append(pred_sv_detection)\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\nimport pickle\n\n# Save the detections\nwith open(\"detections.pkl\", \"wb\") as f:\n    pickle.dump({\"targets\": targets, \"predictions\": predictions}, f)\n\n# Load the detections\nwith open(\"detections.pkl\", \"rb\") as f:\n    data = pickle.load(f)\ntargets = data[\"targets\"]\npredictions = data[\"predictions\"]\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\n# Compute mAP50 using supervision\nmetrics = sv.metrics.MeanAveragePrecision()\nresults = metrics.update(targets, predictions).compute()\nmAP50_supervision = results.map50\nprint(f\"mAP50 using supervision: {mAP50_supervision:.4f}\")\n\n\nmAP50 using supervision: 0.9018\n\n\n\n\nCode\n# Compute mAP50 using my implementation\nall_gt_boxes, all_pred_boxes, all_scores = [], [], []\n\nfor target, prediction in zip(targets, predictions):\n    all_gt_boxes.append(target.xyxy)\n    all_pred_boxes.append(prediction.xyxy)\n    all_scores.append(prediction.confidence)\n\nprecisions, recalls = compute_precision_recall(all_gt_boxes, all_pred_boxes, all_scores, image_size, iou_threshold=0.5)\nmAP50_mine = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f\"mAP50 using my implementation: {mAP50_mine:.4f}\")\n\n\nmAP50 using my implementation: 0.8930\n\n\nTODO:\nCreate a table of Precision, Recall and F1-scores where rows are IoU thresholds [0.1, 0.3, 0.5, 0.7, 0.9] and columns are confidence thresholds [0.1, 0.3, 0.5, 0.7, 0.9]\n(Hint use supervision.metrics.ConfusionMatrix to get the confusion matrix and get TP, FP and FN from it to compute the P, R and F-1)"
  },
  {
    "objectID": "posts/ml-publication-trends/ml_publication_trends.html",
    "href": "posts/ml-publication-trends/ml_publication_trends.html",
    "title": "ML Publication Trends",
    "section": "",
    "text": "CS 328 Writing Assignment - 2025\nGroup Members: Devansh Lodha (23110091), Tejas Lohia (23110335), Mohit (23110207), Tanishq Chaudhari (23110329), Shardul Junagade (23110297)"
  },
  {
    "objectID": "posts/ml-publication-trends/ml_publication_trends.html#introduction",
    "href": "posts/ml-publication-trends/ml_publication_trends.html#introduction",
    "title": "ML Publication Trends",
    "section": "1. Introduction",
    "text": "1. Introduction\nThe field of Artificial Intelligence (AI) has experienced exponential growth over the past two decades. Tracking publications at top-tier Machine Learning (ML) conferences like NeurIPS, ICML, and ICLR provides a valuable lens through which to observe this growth, identify leading institutions and countries, and understand shifting trends, such as the increasing role of industry research.\nThis report analyzes a dataset compiling paper titles, authors, and affiliations from NeurIPS (2006-2024), ICML (2017-2024), and ICLR (2018-2024, excluding 2020). The primary challenge lies in the unstructured nature of the original affiliation data. To address this, a Large Language Model (LLM) approach was employed to extract and standardize institution names, countries, and affiliation types (Academia/Industry).\nThe analysis follows the structure outlined in the assignment: 1. Summarize the prepared data and provide an overview using visualizations. 2. Posit hypotheses regarding growth, geographic shifts, industry involvement, and research concentration. 3. Quantify and analyze the data to settle these hypotheses. 4. Discuss the findings, limitations, and conclusions.\nAll analyses and visualizations were performed using Python with pandas for data manipulation and Plotly for interactive plotting.\n\n\nCode\nimport pandas as pd\nimport plotly.io as pio\nimport plotly.express as px\nfrom sklearn.cluster import KMeans \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.decomposition import PCA\n\n# Plotly setup for notebook display\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\npio.templates.default = \"plotly_white\""
  },
  {
    "objectID": "posts/ml-publication-trends/ml_publication_trends.html#data-loading-and-affiliation-processing",
    "href": "posts/ml-publication-trends/ml_publication_trends.html#data-loading-and-affiliation-processing",
    "title": "ML Publication Trends",
    "section": "2. Data Loading and Affiliation Processing",
    "text": "2. Data Loading and Affiliation Processing\nThe analysis begins with the raw dataset containing publication metadata. A critical challenge is the Affiliation column, which contains unstructured, highly variable text strings. Examples include:\n\n\"Dept. of Computer Science, Stanford University, CA, USA and Google Research\"\n\"MIT\"\n\"Institute for Neuroinformatics, University of Zurich and ETH Zurich\"\n\"Tsinghua University\"\n\nDirect analysis based on this raw text is infeasible due to inconsistency, ambiguity (e.g., “MIT”), and the presence of multiple institutions within single strings. To enable meaningful analysis of institutional and geographic trends, we need to extract structured information:\n\nStandardized Institution Name: Map variations to a common name (e.g., “MIT” -&gt; “Massachusetts Institute of Technology”, “Google Brain” -&gt; “Google”).\nCountry: Identify or infer the country for each institution.\nType: Classify the institution as “Academia” or “Industry”.\n\nGiven the scale (12,887 unique affiliations) and complexity, a Large Language Model (LLM) approach was chosen for this extraction task.\n\n\nCode\n# Load the RAW dataset\ndf_raw = pd.read_csv('papers.csv')\n\nprint(\"Raw DataFrame shape:\", df_raw.shape)\nprint(\"Number of unique affiliations:\", df_raw['Affiliation'].nunique())\ndf_raw.head()\n\n\n\n2.1 LLM-Based Information Extraction Methodology\nWe utilize the mistralai/Mistral-Small-24B-Instruct-2501 model via the vLLM inference engine for efficient batch processing. The core idea is to provide each unique affiliation string to the LLM within a carefully crafted prompt, instructing it to return a structured JSON list containing the desired information for all institutions mentioned.\nSteps We Followed:\n\nIdentify Unique Affiliations: Extract the unique non-null affiliation strings from the raw data.\nPrompt Engineering: Develop a detailed prompt template that:\n\nClearly defines the task (extract institution, country, type).\nInstructs the model to handle multiple institutions per string.\nProvides examples of desired normalization (e.g., MIT, Google variations).\nSpecifies the output format as a JSON list of objects ([{ \"institution\": ..., \"country\": ..., \"type\": ... }, ...]).\nRequests country inference where possible, using null if uncertain.\nUses TEMPERATURE=0.0 for deterministic output suitable for extraction.\n(The final prompt template is shown within the code cell below).\n\nBatch Inference (vLLM): Generate responses for all unique affiliations in batches using vLLM for performance. (Note: Running the next code cell required significant GPU resources (like an A100 40GB used via Ola Krutim credits offerred to IIT Gandhinagar) and the vLLM library).\nParsing and Validation: Parse the LLM’s text output as JSON. Validate that the output is a list of dictionaries, with each dictionary containing the required keys. Store the parsed list, the raw model output, and a parse status for each affiliation.\nMapping and Exploding: Map the structured results back to the original DataFrame and then use the pandas explode function to create one row per author-institution pair.\n\n\n\nCode\n# Import necessary libraries for this block assuming seperate execution\nimport pandas as pd\nimport json\nfrom vllm import LLM, SamplingParams\n\n# Configuration\nMODEL_NAME = \"mistralai/Mistral-Small-24B-Instruct-2501\" # Your specified model\nTENSOR_PARALLEL_SIZE = 1\nMAX_NEW_TOKENS = 800 # Increased for potentially long JSON lists\nTEMPERATURE = 0.0\nGPU_MEMORY_UTILIZATION = 0.9 # Adjust if needed\n\n# Get Unique Affiliations\nunique_affiliations = df_raw['Affiliation'].dropna().unique().tolist()\n\n# Define Prompt Template\n# Using f-strings requires escaping the literal braces in the JSON examples with double braces {{ }}\nPROMPT_TEMPLATE = \"\"\"Your task is to analyze the affiliation string provided and extract detailed information about all mentioned institutions.\n\nFollow these instructions carefully:\n1.  Identify all distinct institutions mentioned in the affiliation string.\n2.  For each institution, determine its full standardized name, its country, and its type (\"Academia\" or \"Industry\").\n3.  **Normalization:** Standardize common institution names. For example:\n    *   \"MIT\", \"M.I.T.\" should become \"Massachusetts Institute of Technology\".\n    *   \"Google Research\", \"Google Brain\", \"DeepMind\" should become \"Google\".\n    *   \"UC Berkeley\" should become \"University of California, Berkeley\".\n    *   Use the full official name where possible.\n4.  **Country Inference:** If the country is not explicitly mentioned, try to infer it from the institution name, city, or state. If the country cannot be determined reliably, use `null`.\n5.  **Output Format:** Return the information as a **JSON list**. Each element in the list should be a JSON object containing the keys \"institution\", \"country\", and \"type\" for one identified institution.\n6.  **Output ONLY the JSON list**, nothing else before or after.\n\nExample 1:\nAffiliation: \"Dept. of Computer Science, Stanford University, CA, USA and Google Research\"\nJSON Output:\n[\n  {{\"institution\": \"Stanford University\", \"country\": \"USA\", \"type\": \"Academia\"}},\n  {{\"institution\": \"Google\", \"country\": \"USA\", \"type\": \"Industry\"}}\n]\n\nExample 2:\nAffiliation: \"Université de Montréal, Mila - Quebec AI Institute\"\nJSON Output:\n[\n  {{\"institution\": \"Université de Montréal\", \"country\": \"Canada\", \"type\": \"Academia\"}},\n  {{\"institution\": \"Mila - Quebec AI Institute\", \"country\": \"Canada\", \"type\": \"Academia\"}}\n]\n\nExample 3:\nAffiliation: \"Vrije Universiteit Brussel\"\nJSON Output:\n[\n  {{\"institution\": \"Vrije Universiteit Brussel\", \"country\": \"Belgium\", \"type\": \"Academia\"}}\n]\n\nNow, analyze the following affiliation:\n\nAffiliation: \"{affiliation}\"\n\nJSON Output:\n\"\"\"\n\n# Prepare Prompts\nprompts = [PROMPT_TEMPLATE.format(affiliation=aff) for aff in unique_affiliations]\n\n# Initialize vLLM\n\nllm = LLM(\n    model=MODEL_NAME,\n    tokenizer_mode=\"mistral\",\n    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n    trust_remote_code=True\n)\n\nresults_map = {} # Initialize results map\n\n\n# Define Sampling Parameters\nsampling_params = SamplingParams(\n    max_tokens=MAX_NEW_TOKENS,\n    temperature=TEMPERATURE,\n)\n\n# Run Batch Inference\noutputs = llm.generate(prompts, sampling_params)\n\n# Process Outputs\nparse_errors = 0\nfor i, output in enumerate(outputs):\n    original_affiliation = unique_affiliations[i]\n    generated_text = output.outputs[0].text.strip()\n\n    result_entry = {\n        'StructuredOutput': None,\n        'ModelOutput': generated_text,\n        'ParseStatus': 'Error: Unknown'\n    }\n\n    cleaned_text = generated_text\n    if cleaned_text.startswith(\"```json\"):\n        cleaned_text = cleaned_text[len(\"```json\"):].strip()\n    if cleaned_text.endswith(\"```\"):\n        cleaned_text = cleaned_text[:-len(\"```\")].strip()\n    if not cleaned_text:\n            raise ValueError(\"Cleaned text is empty\")\n\n    extracted_data = json.loads(cleaned_text)\n\n    if isinstance(extracted_data, list) and all(isinstance(item, dict) for item in extracted_data):\n        all_items_valid = True\n        for item in extracted_data:\n            if not isinstance(item.get(\"institution\"), str) or \\\n                not isinstance(item.get(\"type\"), str) or \\\n                not (isinstance(item.get(\"country\"), str) or item.get(\"country\") is None):\n                    all_items_valid = False\n                    print(f\"Warning: Item in list lacks expected keys/types for affiliation '{original_affiliation}'. Item: {item}\")\n                    break\n        if all_items_valid:\n            result_entry['StructuredOutput'] = extracted_data\n            result_entry['ParseStatus'] = 'Success'\n        else:\n            result_entry['ParseStatus'] = 'Error: Invalid Item Structure'\n            parse_errors += 1\n\n    results_map[original_affiliation] = result_entry\n\n# Clean up GPU memory\ndel llm\nif 'torch' in locals() or 'torch' in globals():\n    import torch\n    torch.cuda.empty_cache()\nprint(\"GPU cache cleared.\")\n\n\n\n\n2.2 Processing LLM Results & Data Structuring\nThe raw text outputs from the LLM containing JSON lists are parsed and validated. Key challenges during this phase include handling malformed JSON, ensuring the output is correctly structured as a list of dictionaries, and managing cases where the model failed to extract information or adhere to the format.\nThe results_map dictionary now holds the processing status and structured output (or errors) for each unique affiliation. We map this back to the original DataFrame and then “explode” the rows associated with multiple institutions to create the final analysis-ready DataFrame (df), where each row corresponds to a single author-institution link. * Author (5 NaNs): A very small number of entries had missing author names. Given the large dataset size and the focus on aggregate trends, these rows were removed. * Affiliation (3999 NaNs): This is the original raw string and not used directly in most analyses. Missing values here are expected. * Institution (4028 NaNs): These represent instances where the LLM could not determine a standardized institution (including original affiliation NaNs and parsing issues). For institution-specific analyses, these will be treated as ‘Unknown’. * Country (8264 NaNs): Includes original NaNs and cases where country couldn’t be reliably inferred. This is expected due to variability in affiliation strings. These will be treated as ‘Unknown’ for geographic analysis. * Type (68 NaNs): A small number. We ensure consistency: if Institution is ‘Unknown’, Type should also be ‘Unknown’. Remaining NaNs are marked as ‘Unknown’. Missing or invalid extracted data is represented by NaN or specific ‘Unknown’ placeholders.\n\n\nCode\n# Map results back to the raw DataFrame\ndef get_result_data(affiliation, key, default=None):\n    # Handle potential NaN affiliations during mapping\n    if pd.isna(affiliation):\n        return default\n    result = results_map.get(affiliation)\n    if isinstance(result, dict):\n        return result.get(key, default)\n    return default\n\ndf_raw['StructuredOutput'] = df_raw['Affiliation'].apply(lambda aff: get_result_data(aff, 'StructuredOutput'))\ndf_raw['ModelOutput_LLM'] = df_raw['Affiliation'].apply(lambda aff: get_result_data(aff, 'ModelOutput')) # Use different name\ndf_raw['ParseStatus'] = df_raw['Affiliation'].apply(lambda aff: get_result_data(aff, 'ParseStatus', 'Error: Mapping Failed'))\n\n# Clean up and Prepare for Exploding\ndef clean_structured_output(row):\n    # Handle NaN in Affiliation column itself\n    if pd.isna(row['Affiliation']):\n        return [] # Treat NaN affiliation as having no structured output\n\n    if row['ParseStatus'] != 'Success' and row['StructuredOutput'] is None:\n        return []\n    elif row['StructuredOutput'] is None:\n            if row['ParseStatus'] == 'Error: Mapping Failed':\n                # Affiliation was NaN or somehow missed in unique list -&gt; no output expected\n                return []\n            else: # ParseStatus indicates error, but output is None -&gt; Treat as empty\n                return []\n    elif isinstance(row['StructuredOutput'], list):\n        # Filter out any potential non-dict items within the list, although the parser should prevent this\n        return [item for item in row['StructuredOutput'] if isinstance(item, dict)]\n    else: # Should not happen if parser worked, but fallback\n        return []\n\ndf_raw['StructuredOutput'] = df_raw.apply(clean_structured_output, axis=1)\n\n# Add placeholder for empty lists to keep rows during explode\ndf_raw['StructuredOutput_Explode'] = df_raw['StructuredOutput'].apply(lambda x: [None] if not x else x)\n\n# Explode\ndf_exploded = df_raw.explode('StructuredOutput_Explode', ignore_index=True)\n\n# Extract Final Columns\ndef safe_get(data, key):\n    if isinstance(data, dict):\n        return data.get(key)\n    return None # Handles the None placeholder\n\ndf_exploded['Institution'] = df_exploded['StructuredOutput_Explode'].apply(lambda x: safe_get(x, 'institution'))\ndf_exploded['Country'] = df_exploded['StructuredOutput_Explode'].apply(lambda x: safe_get(x, 'country'))\ndf_exploded['Type'] = df_exploded['StructuredOutput_Explode'].apply(lambda x: safe_get(x, 'type'))\n\n# Final DataFrame Assembly\n# Select and rename columns\ndf_final = df_exploded[['Conference', 'Year', 'Title', 'Author', 'Affiliation', # Original cols\n                        'Institution', 'Country', 'Type', # New extracted cols\n                        'ModelOutput_LLM', 'ParseStatus']].copy() # Metadata cols\n\n# Handle NaNs created during processing\ndf_final.dropna(subset=['Author'], inplace=True) # Should already be done if df_raw was used\ndf_final['Institution'] = df_final['Institution'].fillna(\"Unknown Institution\")\ndf_final['Country'] = df_final['Country'].fillna(\"Unknown Country\")\n# Ensure Type consistency (important AFTER extraction)\ndf_final.loc[df_final['Institution'] == \"Unknown Institution\", 'Type'] = \"Unknown Type\"\ndf_final['Type'] = df_final['Type'].fillna(\"Unknown Type\") # Catch any remaining NaNs\n\n# Ensure Year is numeric\ndf_final['Year'] = pd.to_numeric(df_final['Year'])\n\n# Rename df_final to df to match subsequent code cells\ndf = df_final.copy()\n\n\nYou can download the processed dataframe used further in this analysis here: Download Dataset CSV"
  },
  {
    "objectID": "posts/ml-publication-trends/ml_publication_trends.html#data-summary-and-overview",
    "href": "posts/ml-publication-trends/ml_publication_trends.html#data-summary-and-overview",
    "title": "ML Publication Trends",
    "section": "3. Data Summary and Overview",
    "text": "3. Data Summary and Overview\nWe begin by summarizing the key characteristics of the prepared dataset and visualizing the overall trends.\n\n\nCode\n# Calculations for Summary\nunique_papers_df = df[['Conference', 'Year', 'Title']].drop_duplicates()\ntotal_unique_papers = len(unique_papers_df)\ntotal_authorship_instances = len(df)\ntime_frames = df.groupby('Conference')['Year'].agg(['min', 'max'])\noverall_min_year = df['Year'].min()\noverall_max_year = df['Year'].max()\nunknown_institutions_count = (df['Institution'] == 'Unknown Institution').sum()\nunknown_countries_count = (df['Country'] == 'Unknown Country').sum()\nunknown_types_count = (df['Type'] == 'Unknown Type').sum()\nperc_unknown_inst = (unknown_institutions_count / total_authorship_instances) * 100\nperc_unknown_country = (unknown_countries_count / total_authorship_instances) * 100\nperc_unknown_type = (unknown_types_count / total_authorship_instances) * 100\n\n# Store stats for Markdown\nsummary_stats = {\n    \"Total Unique Papers\": total_unique_papers,\n    \"Total Authorship Instances\": total_authorship_instances,\n    \"Overall Years\": f\"{overall_min_year}-{overall_max_year}\",\n    \"% Unknown Institutions\": f\"{perc_unknown_inst:.2f}%\",\n    \"% Unknown Countries\": f\"{perc_unknown_country:.2f}%\",\n    \"% Unknown Types\": f\"{perc_unknown_type:.2f}%\"\n}\n\n# Plot 1: Unique Papers per Year\nprint(\"\\nPlot 1: Unique Papers per Year\")\npapers_per_year = unique_papers_df.groupby('Year').size().reset_index(name='UniquePaperCount')\nfig1 = px.line(papers_per_year,\n              x='Year',\n              y='UniquePaperCount',\n              markers=True,\n              title='Total Unique Papers per Year (Across All Conferences)',\n              labels={'UniquePaperCount': 'Number of Unique Papers', 'Year': 'Year'})\nfig1.update_layout(hovermode=\"x unified\")\nfig1.show()\n\n# Plot 2: Authorship Instances per Year\nprint(\"Plot 2: Authorship Instances per Year\")\ninstances_per_year = df.groupby('Year').size().reset_index(name='AuthorshipInstanceCount')\nfig2 = px.line(instances_per_year,\n              x='Year',\n              y='AuthorshipInstanceCount',\n              markers=True,\n              title='Total Authorship Instances per Year (Across All Conferences)',\n              labels={'AuthorshipInstanceCount': 'Number of Authorship Instances', 'Year': 'Year'})\nfig2.update_layout(hovermode=\"x unified\")\nfig2.show()\n\n# Plot 3: Total Authorship Instances per Conference\nprint(\"Plot 3: Total Authorship Instances per Conference\")\nconf_counts = df['Conference'].value_counts().reset_index()\nconf_counts.columns = ['Conference', 'Count'] # Rename columns for clarity\nfig3 = px.bar(conf_counts,\n             x='Conference',\n             y='Count',\n             color='Conference',\n             title='Total Authorship Instances per Conference (Overall)',\n             labels={'Count': 'Number of Authorship Instances', 'Conference': 'Conference'},\n             text_auto=True)\nfig3.update_layout(showlegend=False)\nfig3.show()\n\n# Plot 4: Top 15 Countries by Total Authorship Instances\nprint(\"Plot 4: Top 15 Countries\")\ncountry_df_filtered = df[df['Country'] != 'Unknown Country']\ncountry_counts = country_df_filtered['Country'].value_counts().head(15).reset_index()\ncountry_counts.columns = ['Country', 'Count']\ncountry_counts = country_counts.sort_values(by='Count', ascending=True) # Sort for horizontal bar\n\nfig4 = px.bar(country_counts,\n             x='Count',\n             y='Country',\n             orientation='h',\n             title='Top 15 Countries by Total Authorship Instances (Overall)',\n             labels={'Count': 'Number of Authorship Instances', 'Country': 'Country'},\n             text='Count')\nfig4.update_layout(yaxis={'categoryorder':'total ascending'})\nfig4.update_traces(textposition='outside')\nfig4.show()\n\n# Plot 5: Top 15 Institutions by Total Authorship Instances\nprint(\"Plot 5: Top 15 Institutions\")\ninst_df_filtered = df[df['Institution'] != 'Unknown Institution']\ninst_counts = inst_df_filtered['Institution'].value_counts().head(15).reset_index()\ninst_counts.columns = ['Institution', 'Count']\ninst_counts = inst_counts.sort_values(by='Count', ascending=True) # Sort for horizontal bar\n\nfig5 = px.bar(inst_counts,\n             x='Count',\n             y='Institution',\n             orientation='h',\n             title='Top 15 Institutions by Total Authorship Instances (Overall)',\n             labels={'Count': 'Number of Authorship Instances', 'Institution': 'Institution'},\n             text='Count')\nfig5.update_layout(yaxis={'categoryorder':'total ascending'})\nfig5.update_traces(textposition='outside')\nfig5.show()\n\n# Plot 6: Overall Distribution by Type (Academia/Industry)\nprint(\"Plot 6: Distribution by Type\")\ntype_df_filtered = df[df['Type'].isin(['Academia', 'Industry'])] # Filter only known types\ntype_counts = type_df_filtered['Type'].value_counts().reset_index()\ntype_counts.columns = ['Type', 'Count']\n\nif not type_counts.empty:\n    fig6 = px.pie(type_counts,\n                 names='Type',\n                 values='Count',\n                 title='Overall Distribution of Authorship Instances by Type',\n                 hole=0.3)\n    fig6.update_traces(textinfo='percent+label', pull=[0.05, 0.05])\n    fig6.show()\nelse:\n    print(\"No known types ('Academia'/'Industry') found to generate distribution plot.\")\n\n\n\nPlot 1: Unique Papers per Year\n\n\n                            \n                                            \n\n\nPlot 2: Authorship Instances per Year\n\n\n                            \n                                            \n\n\nPlot 3: Total Authorship Instances per Conference\n\n\n                            \n                                            \n\n\nPlot 4: Top 15 Countries\n\n\n                            \n                                            \n\n\nPlot 5: Top 15 Institutions\n\n\n                            \n                                            \n\n\nPlot 6: Distribution by Type\n\n\n                            \n                                            \n\n\n\n3.1 Summary of Dataset Characteristics\nThe prepared dataset covers the period 2006-2024 across NeurIPS, ICML, and ICLR (excluding 2020). It contains 38510 unique papers and 18345 authorship instances (representing individual author-institution links).\n\nData Completeness: After processing, the remaining percentages of unmapped entries are:\n\nInstitutions: 2.19%\nCountries: 4.50%\nTypes: 2.21%\n\nOverview Trends (from plots above):\n\nBoth unique paper counts and total authorship instances show a strong upward trend over the years, confirming significant growth in research output at these venues.\nNeurIPS has the highest overall volume of authorship instances, followed by ICML and ICLR.\nGeographically, the USA and China are the dominant contributors, with the UK, Canada, and Germany also having significant presence.\nInstitutionally, Google, Stanford, MIT, Tsinghua, and CMU lead in overall contributions.\nAffiliation types are predominantly Academia (around 70-80%), but Industry forms a substantial minority."
  },
  {
    "objectID": "posts/ml-publication-trends/ml_publication_trends.html#hypotheses-analysis-and-findings",
    "href": "posts/ml-publication-trends/ml_publication_trends.html#hypotheses-analysis-and-findings",
    "title": "ML Publication Trends",
    "section": "4. Hypotheses, Analysis and Findings",
    "text": "4. Hypotheses, Analysis and Findings\nBased on the dataset and initial overview, we formulate hypotheses to investigate specific trends.\n\nH1: Overall Growth\nHypothesis: The volume of AI research published at these top conferences (measured by unique papers and authorship instances) has increased significantly over the period covered by the dataset.\nQuantification: Plotting unique papers per year and total authorship instances per year (Plots 1 & 2 generated in Section 3).\nFindings: Plots 1 and 2 clearly show a strong, near-exponential increase in both the number of unique papers and the total volume of authorship instances over the years, particularly accelerating after ~2015.\nConclusion: H1 is strongly supported. The data confirms significant growth in publication volume at these top AI conferences.\n\n\nH2: Geographic Shifts\nHypothesis: While North America (USA/Canada) has historically dominated, contributions from East Asia (particularly China) have shown the most rapid growth in recent years, significantly increasing their share of publications.\nQuantification: Calculate the percentage share of total yearly authorship instances for key countries over time. Filter out “Unknown Country” instances before calculating percentages.\n\n\nCode\n# Filter out unknown countries\ngeo_df = df[df['Country'] != 'Unknown Country'].copy()\n\n# Calculate counts per year and country\ncountry_yearly_counts = geo_df.groupby(['Year', 'Country']).size().reset_index(name='Count')\n\n# Calculate total counts per year (only considering known countries)\ntotal_yearly_counts = country_yearly_counts.groupby('Year')['Count'].sum().reset_index(name='TotalYearlyCount')\n\n# Merge totals back to calculate percentages\ncountry_yearly_perc = pd.merge(country_yearly_counts, total_yearly_counts, on='Year')\ncountry_yearly_perc['Percentage'] = (country_yearly_perc['Count'] / country_yearly_perc['TotalYearlyCount']) * 100\n\n# Select key countries/regions to plot\nkey_countries = ['USA', 'China', 'Canada', 'UK', 'Germany', 'France', 'India', 'Japan', 'South Korea', 'Switzerland']\n\n# Filter data for plotting\nplot_data_h2 = country_yearly_perc[country_yearly_perc['Country'].isin(key_countries)]\n\n# Create the interactive line plot\nfig_h2 = px.line(plot_data_h2,\n                 x='Year',\n                 y='Percentage',\n                 color='Country',\n                 markers=True,\n                 title='Percentage Share of Authorship Instances by Key Countries Over Time',\n                 labels={'Percentage': '% of Total Yearly Instances', 'Year': 'Year'},\n                 hover_data={'Count': True, 'Percentage': ':.2f%'})\n\nfig_h2.update_layout(hovermode=\"x unified\")\nfig_h2.show()\n\n\n                            \n                                            \n\n\nFindings: The plot reveals dynamic shifts in geographic contributions: * The USA’s share, while consistently high, has declined from over 60% in the early years to around 35-40% more recently. * China’s share shows dramatic growth, rising from less than 5% before 2014 to become the second-largest contributor, exceeding 25% in recent years, challenging the USA’s dominance. * Canada and the UK maintain significant shares, often fluctuating but generally ranking 3rd/4th or 4th/5th. * Germany, France, and Switzerland show relatively stable contributions within the top 10. * South Korea and India demonstrate noticeable growth in their percentage share, particularly in the later years.\nConclusion: H2 is strongly supported. While North America (primarily USA, with Canada stable) remains a major force, East Asia (driven overwhelmingly by China’s rapid ascent) has significantly increased its share, confirming the geographic shift.\n\n\nH3: Rise of Industry\nHypothesis: The proportion of research contributions from industry-affiliated authors has steadily increased over time compared to academic institutions.\nQuantification: Calculate the percentage share of total yearly authorship instances attributed to ‘Industry’ over time. Filter out “Unknown Type” instances.\n\n\nCode\n# Filter out unknown types\ntype_df = df[df['Type'].isin(['Academia', 'Industry'])].copy() # Use isin for clarity\n\n# Calculate counts per year and type\ntype_yearly_counts = type_df.groupby(['Year', 'Type']).size().reset_index(name='Count')\n\n# Calculate total counts per year (known types only)\ntotal_yearly_type_counts = type_yearly_counts.groupby('Year')['Count'].sum().reset_index(name='TotalYearlyCount')\n\n# Merge totals back to calculate percentages\ntype_yearly_perc = pd.merge(type_yearly_counts, total_yearly_type_counts, on='Year')\ntype_yearly_perc['Percentage'] = (type_yearly_perc['Count'] / type_yearly_perc['TotalYearlyCount']) * 100\n\n# Filter for Industry type\nplot_data_h3 = type_yearly_perc[type_yearly_perc['Type'] == 'Industry']\n\n# Create the interactive line plot\nfig_h3 = px.line(plot_data_h3,\n                 x='Year',\n                 y='Percentage',\n                 markers=True,\n                 title='Percentage Share of Authorship Instances from Industry Over Time',\n                 labels={'Percentage': '% of Total Yearly Instances from Industry', 'Year': 'Year'})\n\nfig_h3.update_layout(hovermode=\"x unified\")\nfig_h3.show()\n\n\n                            \n                                            \n\n\nFindings: The plot demonstrates a clear and substantial upward trend in the percentage of authorship instances affiliated with Industry. Accelerating after 2015 and reaching approximately 30-35% in the most recent years.\nConclusion: H3 is strongly supported. Industry participation in research publications at these top AI conferences has significantly increased over time.\n\n\nH4: Institutional Concentration\nHypothesis: A relatively small number of ‘elite’ institutions (both academic and industrial) consistently account for a large percentage of the total publications, indicating a concentration of top AI research.\nQuantification: Identify the Top \\(N=20\\) institutions based on overall authorship instances. Calculate the combined percentage share of total yearly authorship instances held by these Top \\(N\\) institutions over time. Filter out “Unknown Institution”.\n\n\nCode\n# Filter out unknown institutions\ninst_df = df[df['Institution'] != 'Unknown Institution'].copy()\n\n# Part 1: Identify Top N Institutions Overall\nN = 20\ntop_n_institutions = inst_df['Institution'].value_counts().head(N).index.tolist()\nprint(f\"Top {N} Institutions (Overall):\")\nprint(top_n_institutions)\n\n# Part 2: Calculate Yearly Concentration Percentage\ninst_yearly_counts = inst_df.groupby(['Year', 'Institution']).size().reset_index(name='Count')\ntotal_yearly_inst_counts = inst_yearly_counts.groupby('Year')['Count'].sum().reset_index(name='TotalYearlyCount')\ntop_n_yearly_counts = inst_yearly_counts[inst_yearly_counts['Institution'].isin(top_n_institutions)]\ntop_n_yearly_sum = top_n_yearly_counts.groupby('Year')['Count'].sum().reset_index(name=f'Top{N}_Count')\nconcentration_df = pd.merge(total_yearly_inst_counts, top_n_yearly_sum, on='Year', how='left').fillna(0) # Use how='left' and fillna(0)\nconcentration_df[f'Top{N}_Percentage'] = (concentration_df[f'Top{N}_Count'] / concentration_df['TotalYearlyCount']) * 100\n\n# Visualization: Concentration Trend\nfig_h4_conc = px.line(concentration_df,\n                      x='Year',\n                      y=f'Top{N}_Percentage',\n                      markers=True,\n                      title=f'Percentage Share of Authorship Instances by Top {N} Institutions Over Time',\n                      labels={f'Top{N}_Percentage': f'% Share Held by Top {N}', 'Year': 'Year'})\n\nfig_h4_conc.update_layout(hovermode=\"x unified\")\nfig_h4_conc.show()\n\n\nTop 20 Institutions (Overall):\n['Google', 'Stanford University', 'Massachusetts Institute of Technology', 'Tsinghua University', 'Carnegie Mellon University', 'Microsoft', 'University of California, Berkeley', 'Peking University', 'University of Oxford', 'University of Texas at Austin', 'Shanghai Jiao Tong University', 'Princeton University', 'Facebook', 'University of California, Los Angeles', 'Zhejiang University', 'University of Washington', 'Georgia Institute of Technology', 'ETH Zurich', 'National University of Singapore', 'University of Cambridge']\n\n\n                            \n                                            \n\n\nFindings: The list reveals the dominant institutions overall (Google, Stanford, MIT, Tsinghua, CMU, Microsoft, Berkeley, etc.). The concentration plot shows that the Top 20 institutions consistently account for a significant portion (roughly 35% to over 50%) of the total authorship instances. While there are fluctuations, the share held by the top institutions appears to have generally increased, especially peaking around 2016-2020, before maybe slightly decreasing in the last couple of years shown.\nConclusion: H4 is supported. A relatively small number of elite institutions contribute a disproportionately large share of publications, indicating significant concentration. The level of concentration seems to have increased over time, although it might have plateaued or slightly decreased very recently.\n\n\nH5: Conference Profile Differences\nHypothesis: The dominant contributing countries and the balance between Academia and Industry differ significantly across the three conferences (NeurIPS, ICML, ICLR).\nQuantification: Calculate percentage share by country and type per conference. Visualize using faceted and grouped bar charts.\n\n\nCode\nN_TOP_COUNTRIES = 10\n\n# Part 1: Geographic Distribution per Conference\nprint(f\"\\nGeographic Distribution per Conference (Top {N_TOP_COUNTRIES})\")\ngeo_conf_df = df[df['Country'] != 'Unknown Country'].copy()\nconf_country_counts = geo_conf_df.groupby(['Conference', 'Country']).size().reset_index(name='Count')\ntotal_conf_counts = conf_country_counts.groupby('Conference')['Count'].sum().reset_index(name='TotalConferenceCount')\nconf_country_perc = pd.merge(conf_country_counts, total_conf_counts, on='Conference')\nconf_country_perc['Percentage'] = (conf_country_perc['Count'] / conf_country_perc['TotalConferenceCount']) * 100\n# Get Top N countries FOR EACH conference using indices from nlargest\ntop_indices = conf_country_perc.groupby('Conference')['Count'].nlargest(N_TOP_COUNTRIES).reset_index()['level_1']\ntop_countries_per_conf = conf_country_perc.loc[top_indices]\n\nfig_h5_geo = px.bar(top_countries_per_conf,\n                    x='Country', y='Percentage', color='Conference', facet_col='Conference',\n                    title=f'Top {N_TOP_COUNTRIES} Contributing Countries by Conference (% of Total Instances per Conference)',\n                    labels={'Percentage': '% of Conference Instances', 'Country': 'Country'},\n                    category_orders={\"Conference\": [\"NeurIPS\", \"ICML\", \"ICLR\"]}, text='Percentage')\nfig_h5_geo.update_traces(texttemplate='%{text:.1f}%', textposition='outside')\nfig_h5_geo.update_xaxes(matches=None, showticklabels=True, title_text='') # Allow independent x-axes and remove redundant title\nfig_h5_geo.update_layout(showlegend=True)\nfig_h5_geo.add_annotation(x=0.5, y=-0.30, xref='paper', yref='paper', text='Country', showarrow=False, font=dict(size=12))\nfig_h5_geo.show()\n\n# Part 2: Academia vs. Industry Distribution per Conference\nprint(\"\\nAcademia vs. Industry Distribution per Conference\")\ntype_conf_df = df[df['Type'].isin(['Academia', 'Industry'])].copy()\nconf_type_counts = type_conf_df.groupby(['Conference', 'Type']).size().reset_index(name='Count')\ntotal_conf_type_counts = conf_type_counts.groupby('Conference')['Count'].sum().reset_index(name='TotalConferenceCount')\nconf_type_perc = pd.merge(conf_type_counts, total_conf_type_counts, on='Conference')\nconf_type_perc['Percentage'] = (conf_type_perc['Count'] / conf_type_perc['TotalConferenceCount']) * 100\n\nfig_h5_type = px.bar(conf_type_perc,\n                     x='Conference', y='Percentage', color='Type', barmode='group',\n                     title='Academia vs. Industry Distribution by Conference',\n                     labels={'Percentage': '% of Conference Instances', 'Conference': 'Conference', 'Type': 'Institution Type'},\n                     category_orders={\"Conference\": [\"NeurIPS\", \"ICML\", \"ICLR\"], \"Type\": [\"Academia\", \"Industry\"]},\n                     text='Percentage')\nfig_h5_type.update_traces(texttemplate='%{text:.1f}%', textposition='outside')\nfig_h5_type.show()\n\n\n\nGeographic Distribution per Conference (Top 10)\n\n\n                            \n                                            \n\n\n\nAcademia vs. Industry Distribution per Conference\n\n\n                            \n                                            \n\n\nFindings: * Geographic Profiles: While USA and China dominate across all three, the relative ranking and presence of other countries vary. * Academia vs. Industry: ICLR shows the highest percentage of industry contributions (25.3%), followed by ICML (23.5%) and then NeurIPS (21.3%). Conversely, NeurIPS has the highest academic share.\nConclusion: H5 is supported. The conferences exhibit distinct profiles regarding both the detailed geographic distribution beyond the top 2 leaders and the balance between academic and industrial participation, with NeurIPS being the most academic-centric and ICML the most industry-heavy overall.\n\n\nH6: International Collaboration Trends\nHypothesis: The proportion of papers involving international collaboration (authors from more than one country) has increased over the years.\nQuantification: Identify unique papers and count the number of distinct countries per paper. Calculate the percentage of papers per year with \\(&gt;1\\) country.\n\n\nCode\nprint(\"\\nInternational Collaboration Trends\")\n\n# Filter out unknown countries for this analysis\ncollab_df = df[df['Country'] != 'Unknown Country'].copy()\n\n# Group by paper and count unique countries\npaper_countries = collab_df.groupby(['Conference', 'Year', 'Title'])['Country'].nunique().reset_index(name='NumCountries')\n\n# Calculate total number of unique papers per year\nunique_papers_per_year = df[['Conference', 'Year', 'Title']].drop_duplicates().groupby('Year').size().reset_index(name='TotalPapers')\n\n# Identify papers with international collaboration\ninternational_papers = paper_countries[paper_countries['NumCountries'] &gt; 1]\n\n# Count international papers per year\ninternational_papers_per_year = international_papers.groupby('Year').size().reset_index(name='InternationalPapers')\n\n# Merge and calculate percentage\ncollaboration_trend = pd.merge(unique_papers_per_year, international_papers_per_year, on='Year', how='left').fillna(0)\ncollaboration_trend['PercentageInternational'] = (collaboration_trend['InternationalPapers'] / collaboration_trend['TotalPapers']) * 100\n\n# Visualization\nfig_h6 = px.line(collaboration_trend,\n                 x='Year', y='PercentageInternational', markers=True,\n                 title='Percentage of Papers with International Collaboration Over Time',\n                 labels={'PercentageInternational': '% of Papers with &gt;1 Country Represented', 'Year': 'Year'})\nfig_h6.update_layout(hovermode=\"x unified\")\nfig_h6.show()\n\n\n\nInternational Collaboration Trends\n\n\n                            \n                                            \n\n\nFindings: The plot shows a clear and significant upward trend in the percentage of papers involving authors from more than one country. Starting below 40% in the early years, the rate increased steadily with a drop near 2015 nearing 50% in the most recent years.\nConclusion: H6 is supported. International collaboration in AI research published at these top conferences has markedly increased over the analyzed period.\n\n\nH7: Academia-Industry Collaboration Trends\nHypothesis: The proportion of papers involving collaboration between academic and industrial researchers has increased over time.\nQuantification: Identify unique papers and count the number of distinct affiliation types (‘Academia’, ‘Industry’) per paper. Calculate the percentage of papers per year with authors from both types.\n\n\nCode\nprint(\"\\nAcademia-Industry Collaboration Trends\")\n\n# Filter out unknown/other types\nacad_ind_df = df[df['Type'].isin(['Academia', 'Industry'])].copy()\n\n# Group by paper and find the set of unique types\npaper_types = acad_ind_df.groupby(['Conference', 'Year', 'Title'])['Type'].apply(set).reset_index(name='TypeSet')\n\n# Identify papers with mixed collaboration\npaper_types['IsMixedCollaboration'] = paper_types['TypeSet'].apply(lambda x: {'Academia', 'Industry'}.issubset(x))\n\n# Use unique_papers_per_year from H6\n# Count mixed papers per year\nmixed_papers = paper_types[paper_types['IsMixedCollaboration']]\nmixed_papers_per_year = mixed_papers.groupby('Year').size().reset_index(name='MixedPapers')\n\n# Merge and calculate percentage\nacad_ind_trend = pd.merge(unique_papers_per_year, mixed_papers_per_year, on='Year', how='left').fillna(0)\nacad_ind_trend['PercentageMixed'] = (acad_ind_trend['MixedPapers'] / acad_ind_trend['TotalPapers']) * 100\n\n# Visualization\nfig_h7 = px.line(acad_ind_trend,\n                 x='Year', y='PercentageMixed', markers=True,\n                 title='Percentage of Papers with Academia-Industry Collaboration Over Time',\n                 labels={'PercentageMixed': '% of Papers with Both Academia & Industry Authors', 'Year': 'Year'})\nfig_h7.update_layout(hovermode=\"x unified\")\nfig_h7.show()\n\n\n\nAcademia-Industry Collaboration Trends\n\n\n                            \n                                            \n\n\nFindings: Similar to international collaboration, the percentage of papers featuring authors from both academic and industrial institutions shows an upward trend. Starting below 40% in the early years, this form of collaboration increased substantially, particularly after a drop near 2015, reaching nearly 45% in recent years.\nConclusion: H7 is supported. Collaboration between academia and industry, as reflected in co-authored papers at these conferences, has significantly increased over time.\n\n\nH8: Evolution within China (Academia vs. Industry)\nHypothesis: Within China, the balance between academic and industrial contributions has shifted significantly over time.\nQuantification: Filter for China, then plot the percentage share of ‘Academia’ vs. ‘Industry’ instances over time within the country.\n\n\nCode\nTARGET_COUNTRY = 'China'\n\nprint(f\"\\nAcademia vs. Industry Trend within {TARGET_COUNTRY}\")\n\n# Filter for the target country AND known types\ncountry_type_df = df[(df['Country'] == TARGET_COUNTRY) &\n                     (df['Type'].isin(['Academia', 'Industry']))].copy()\n\nif country_type_df.empty:\n    print(f\"No data found for known types in {TARGET_COUNTRY}.\")\nelse:\n    country_type_yearly_counts = country_type_df.groupby(['Year', 'Type']).size().reset_index(name='Count')\n    total_yearly_country_counts = country_type_yearly_counts.groupby('Year')['Count'].sum().reset_index(name='TotalYearlyCount')\n    country_type_yearly_perc = pd.merge(country_type_yearly_counts, total_yearly_country_counts, on='Year')\n    country_type_yearly_perc['Percentage'] = (country_type_yearly_perc['Count'] / country_type_yearly_perc['TotalYearlyCount']) * 100\n\n    fig_h8 = px.line(country_type_yearly_perc,\n                     x='Year', y='Percentage', color='Type', markers=True,\n                     title=f'Academia vs. Industry Share of Authorship Instances from {TARGET_COUNTRY}',\n                     labels={'Percentage': f'% of {TARGET_COUNTRY} Yearly Instances', 'Year': 'Year', 'Type': 'Institution Type'})\n    fig_h8.update_layout(hovermode=\"x unified\")\n    fig_h8.show()\n\n\n\nAcademia vs. Industry Trend within China\n\n\n                            \n                                            \n\n\nFindings: Within China, industry contributions initially dominated almost entirely. Academia’s share shows noticeable growth starting around 2005-2010 and increasing significantly thereafter, reaching around 85% of Chinese contributions in recent years.\nConclusion: H8 is supported. The internal balance in China has shifted, with a marked increase in academia participation in recent years, although academia still holds the majority share.\n\n\nH9: Persistence of Top Institutions\nHypothesis: The institutions ranked in the Top \\(N=20\\) during an earlier period largely remain dominant in a later period.\nQuantification: Define early (2010-2015) and late (2019-2023) periods. Identify Top 20 institutions in each. Calculate the percentage of the early Top 20 also present in the late Top 20.\n\n\nCode\nN_RANK = 20\nEARLY_START, EARLY_END = 2010, 2015\nLATE_START, LATE_END = 2019, 2023\n\nprint(f\"\\nPersistence of Top {N_RANK} Institutions\")\nprint(f\"Early Period: {EARLY_START}-{EARLY_END}, Late Period: {LATE_START}-{LATE_END}\")\n\ninst_persist_df = df[(df['Institution'] != 'Unknown Institution')].copy()\ninst_persist_df['Year'] = pd.to_numeric(inst_persist_df['Year'])\n\nearly_df = inst_persist_df[(inst_persist_df['Year'] &gt;= EARLY_START) & (inst_persist_df['Year'] &lt;= EARLY_END)]\nlate_df = inst_persist_df[(inst_persist_df['Year'] &gt;= LATE_START) & (inst_persist_df['Year'] &lt;= LATE_END)]\n\nif early_df.empty or late_df.empty:\n    print(\"Insufficient data for one or both periods.\")\nelse:\n    top_early = early_df['Institution'].value_counts().head(N_RANK).index.tolist()\n    print(f\"\\nTop {N_RANK} Institutions ({EARLY_START}-{EARLY_END}):\")\n    print(top_early)\n\n    top_late = late_df['Institution'].value_counts().head(N_RANK).index.tolist()\n    print(f\"\\nTop {N_RANK} Institutions ({LATE_START}-{LATE_END}):\")\n    print(top_late)\n\n    persistent_institutions = [inst for inst in top_early if inst in top_late]\n    num_persistent = len(persistent_institutions)\n    persistence_rate = (num_persistent / N_RANK) * 100\n\n    print(f\"\\nNumber of institutions in Top {N_RANK} in both periods: {num_persistent}\")\n    print(f\"Persistence Rate: {persistence_rate:.1f}%\")\n\n\n\nPersistence of Top 20 Institutions\nEarly Period: 2010-2015, Late Period: 2019-2023\n\nTop 20 Institutions (2010-2015):\n['Google', 'Carnegie Mellon University', 'Stanford University', 'Massachusetts Institute of Technology', 'University of California, Berkeley', 'Microsoft', 'University of Texas at Austin', 'Duke University', 'Princeton University', 'Columbia University', 'University of Washington', 'University of Cambridge', 'University of California, San Diego', 'Facebook', 'University College London', 'University of Toronto', 'Technion - Israel Institute of Technology', 'University of Michigan', 'Georgia Institute of Technology', 'Inria']\n\nTop 20 Institutions (2019-2023):\n['Google', 'Stanford University', 'Massachusetts Institute of Technology', 'Carnegie Mellon University', 'Tsinghua University', 'Microsoft', 'University of California, Berkeley', 'Peking University', 'University of Oxford', 'Facebook', 'University of Texas at Austin', 'University of California, Los Angeles', 'Princeton University', 'Georgia Institute of Technology', 'Mila - Quebec AI Institute', 'ETH Zurich', 'University of Washington', 'IBM', 'Amazon', 'Shanghai Jiao Tong University']\n\nNumber of institutions in Top 20 in both periods: 11\nPersistence Rate: 55.0%\n\n\nFindings: Comparing the Top 20 institutions from 2010-2015 with those from 2019-2023, we found that 11 institutions were present in both lists. This yields a persistence rate of 55.0%. Key persistent players include Google, Stanford, MIT, CMU, Microsoft, Berkeley, Princeton, UTexas Austin, UWashington, Georgia Tech. Notable entries into the later top 20 include Tsinghua, Peking, Oxford, UCLA, Mila, ETH Zurich, IBM, Amazon, SJTU, while institutions like Duke, Columbia, Cambridge, UCSD, UCL, Toronto, Technion, Michigan, Inria dropped out of the Top 20 between these periods (though many remain highly ranked).\nConclusion: H9 is partially supported. While there is significant persistence (over half the institutions remain in the top 20), there is also considerable dynamism, with nearly half the top tier changing between the two periods, reflecting the rise of certain institutions (especially from China and industry) and the relative decline of others within this elite group.\n\n\nH10: National Institutional Dominance\nHypothesis: Within the top contributing countries, the national research output is highly concentrated, with the single top institution often accounting for a substantial percentage of that country’s total publications.\nQuantification: Select the Top \\(K=5\\) countries. For each, identify their single top contributing institution overall and calculate the percentage of the country’s total instances originating from that institution. Visualize with a bar chart, adding the institution name to the bar label.\n\n\nCode\nK_TOP_COUNTRIES = 5 # Number of top countries to analyze\n\nprint(f\"\\nInstitutional Dominance within Top {K_TOP_COUNTRIES} Countries\")\n\n# Filter unknowns\ndom_df = df[(df['Country'] != 'Unknown Country') &\n            (df['Institution'] != 'Unknown Institution')].copy()\n\n# Find Top K countries overall\ntop_k_countries = dom_df['Country'].value_counts().head(K_TOP_COUNTRIES).index.tolist()\nprint(f\"Analyzing Top {K_TOP_COUNTRIES} Countries: {top_k_countries}\")\n\n# Filter data for only these top countries\ndom_df_filtered = dom_df[dom_df['Country'].isin(top_k_countries)]\n\n# Calculate total instances per country\ntotal_country_counts = dom_df_filtered.groupby('Country').size().reset_index(name='TotalCountryInstances')\n\n# Find the single top institution WITHIN each of these countries\ntop_inst_per_country = dom_df_filtered.groupby(['Country', 'Institution']).size().reset_index(name='InstCount')\ntop_inst_per_country = top_inst_per_country.loc[top_inst_per_country.groupby('Country')['InstCount'].idxmax()]\ntop_inst_per_country = top_inst_per_country.rename(columns={'InstCount': 'TopInstCount'})\n\n# Merge total country counts and top institution counts\ndominance_summary = pd.merge(total_country_counts, top_inst_per_country[['Country', 'Institution', 'TopInstCount']], on='Country')\n\n# Calculate dominance percentage\ndominance_summary['DominancePercentage'] = (dominance_summary['TopInstCount'] / dominance_summary['TotalCountryInstances']) * 100\n\n# Create the combined text label for the bars\ndominance_summary['BarLabel'] = dominance_summary.apply(\n    lambda row: f\"{row['DominancePercentage']:.1f}%&lt;br&gt;&lt;i&gt;({row['Institution']})&lt;/i&gt;\",\n    axis=1\n)\n\n# Sort by dominance percentage for plotting\ndominance_summary = dominance_summary.sort_values('DominancePercentage', ascending=False)\n\n# Visualization: Bar chart of dominance percentage\nfig_h11 = px.bar(dominance_summary,\n                 x='Country', y='DominancePercentage', color='Country',\n                 title=f'Dominance of Top Institution within Top {K_TOP_COUNTRIES} Countries',\n                 labels={'DominancePercentage': '% National Instances (Top Inst.)', 'Country': 'Country'},\n                 text='BarLabel',\n                 hover_data={'Institution': True, 'TopInstCount': True, 'TotalCountryInstances': True, 'DominancePercentage':':.1f%'})\n\nfig_h11.update_traces(textposition='outside')\nfig_h11.update_layout(showlegend=False, yaxis_title=\"% National Instances (Top Inst.)\")\nfig_h11.show()\n\n\n\nInstitutional Dominance within Top 5 Countries\nAnalyzing Top 5 Countries: ['USA', 'China', 'UK', 'Canada', 'Germany']\n\n\n                            \n                                            \n\n\nFindings: The plot shows the percentage of each top country’s total output contributed by its single largest institution. * The UK shows the highest concentration among the top 5, with the University of Oxford contributing over 27% of its national total. * Canada follows, with the University of Toronto contributing around 18.6%. * China’s output is less concentrated in its top institution (Tsinghua University, ~14.2%) compared to the UK or Canada. * The USA, despite having the highest overall output, shows relatively lower concentration in its top institution (Google, ~13.1%), indicating a broader base of major contributors. * Germany has the lowest concentration among these five, with the Max Planck Institute for Intelligent Systems contributing ~10.0%.\nConclusion: H10 is supported, but with significant variation. While top institutions are clearly important, the degree of national concentration varies. Some countries (UK, Canada) rely more heavily on their single top institution than others (USA, China, Germany) which appear to have a more distributed set of leading contributors within the country.\n\n\nH11: Rise of New Player Countries\nHypothesis: Beyond the established leaders, other countries have emerged as significant contributors in recent years.\nQuantification: Identify countries ranked in the Top 20 post-2018 but not in the Top 20 pre-2017. Plot their absolute contribution trends.\n\n\nCode\nN_RANK_CHECK = 20\nEARLY_END_YEAR = 2016\nLATE_START_YEAR = 2019\n\nprint(f\"\\nRise of New Player Countries (Comparing Pre-{EARLY_END_YEAR+1} and Post-{LATE_START_YEAR-1})\")\n\nrise_df = df[(df['Country'] != 'Unknown Country')].copy()\nrise_df['Year'] = pd.to_numeric(rise_df['Year'])\n\nearly_period_df = rise_df[rise_df['Year'] &lt;= EARLY_END_YEAR]\nlate_period_df = rise_df[rise_df['Year'] &gt;= LATE_START_YEAR]\n\nemerging_countries = []\nif not early_period_df.empty and not late_period_df.empty:\n    top_early_countries = early_period_df['Country'].value_counts().head(N_RANK_CHECK).index.tolist()\n    top_late_countries = late_period_df['Country'].value_counts().head(N_RANK_CHECK).index.tolist()\n    emerging_countries = [c for c in top_late_countries if c not in top_early_countries]\n    print(f\"\\nPotential Emerging Countries (in Top {N_RANK_CHECK} post-{LATE_START_YEAR-1}, not pre-{EARLY_END_YEAR+1}):\")\n    print(emerging_countries)\nelse:\n    print(\"Insufficient data for one or both periods to identify emerging countries.\")\n\nif emerging_countries:\n    country_yearly_counts_all = rise_df.groupby(['Year', 'Country']).size().reset_index(name='Count')\n    plot_data_h12 = country_yearly_counts_all[country_yearly_counts_all['Country'].isin(emerging_countries)]\n\n    fig_h12 = px.line(plot_data_h12,\n                     x='Year', y='Count', color='Country', markers=True,\n                     title=f'Authorship Instance Trends for Emerging Countries',\n                     labels={'Count': 'Number of Authorship Instances', 'Year': 'Year', 'Country': 'Country'})\n    fig_h12.update_layout(hovermode=\"x unified\")\n    fig_h12.show()\nelse:\n    print(\"No emerging countries identified based on the criteria.\")\n\n\n\nRise of New Player Countries (Comparing Pre-2017 and Post-2018)\n\nPotential Emerging Countries (in Top 20 post-2018, not pre-2017):\n['Russia', 'Sweden', 'Denmark']\n\n\n                            \n                                            \n\n\nFindings: Based on the criteria (in Top 20 post-2018, not Top 20 pre-2017), the analysis identifies Russia, Sweden, and Denmark as emerging contributors. The plot shows their absolute authorship instance counts over time. While starting from a low base, all three show an upward trajectory, particularly in the later years, indicating their growing presence in these conferences.\nConclusion: H11 is supported. Countries like Russia, Sweden, and Denmark, while not among the absolute leaders, have demonstrably increased their contributions and presence within the Top 20 landscape in recent years.\n\n\nH12: Conference Concentration Comparison\nHypothesis: The degree to which publications are concentrated within the top institutions differs across the conferences.\nQuantification: Calculate the percentage share held by the Top \\(N=5\\) institutions separately for each conference per year. Plot trends and include Top 5 institutions in hover data.\n\n\nCode\nN_CONC = 5 # Focusing on Top 5 for clarity\n\nprint(f\"\\nTop {N_CONC} Institutional Concentration Across Conferences\")\n\nconc_df = df[df['Institution'] != 'Unknown Institution'].copy()\nconf_inst_yearly_counts = conc_df.groupby(['Year', 'Conference', 'Institution']).size().reset_index(name='Count')\ntotal_conf_yearly_counts = conf_inst_yearly_counts.groupby(['Year', 'Conference'])['Count'].sum().reset_index(name='TotalConfYearlyCount')\n\nconcentration_list = []\nfor year in sorted(conf_inst_yearly_counts['Year'].unique()):\n    for conf in [\"NeurIPS\", \"ICML\", \"ICLR\"]:\n        if conf not in conf_inst_yearly_counts[conf_inst_yearly_counts['Year'] == year]['Conference'].unique():\n            continue\n\n        subset = conf_inst_yearly_counts[(conf_inst_yearly_counts['Year'] == year) & (conf_inst_yearly_counts['Conference'] == conf)]\n\n        if not subset.empty:\n            top_n_inst_subset_df = subset.nlargest(N_CONC, 'Count')\n            top_n_details = \", \".join([f\"{row['Institution']} ({row['Count']})\" for index, row in top_n_inst_subset_df.iterrows()])\n            top_n_count_subset = top_n_inst_subset_df['Count'].sum()\n\n            total_count_subset_row = total_conf_yearly_counts[(total_conf_yearly_counts['Year'] == year) & (total_conf_yearly_counts['Conference'] == conf)]\n            if not total_count_subset_row.empty:\n                 total_count_subset = total_count_subset_row['TotalConfYearlyCount'].iloc[0]\n            else:\n                 total_count_subset = 0\n\n            if total_count_subset &gt; 0:\n                percentage = (top_n_count_subset / total_count_subset) * 100\n                concentration_list.append({\n                    'Year': year, 'Conference': conf, f'Top{N_CONC}_Percentage': percentage,\n                    f'Top{N_CONC}_Institutions': top_n_details })\n            # else: # No need to print warnings during normal run\n            #      print(f\"Warning: Total count is zero for {conf} in {year}\")\n\nconf_concentration_df = pd.DataFrame(concentration_list)\n\nif not conf_concentration_df.empty:\n    fig_h13 = px.line(conf_concentration_df,\n                      x='Year', y=f'Top{N_CONC}_Percentage', color='Conference', markers=True,\n                      title=f'Top {N_CONC} Institution Concentration by Conference Over Time',\n                      labels={f'Top{N_CONC}_Percentage': f'% Share Held by Top {N_CONC}', 'Year': 'Year', 'Conference': 'Conference'},\n                      category_orders={\"Conference\": [\"NeurIPS\", \"ICML\", \"ICLR\"]},\n                      hover_data={f'Top{N_CONC}_Institutions': True, f'Top{N_CONC}_Percentage': ':.1f%'})\n    fig_h13.update_layout(hovermode=\"x unified\")\n    fig_h13.show()\nelse:\n    print(\"Could not calculate conference concentration trends.\")\n\n\n\nTop 5 Institutional Concentration Across Conferences\n\n\n                            \n                                            \n\n\nFindings: The plot compares the percentage share held by the Top 5 institutions across the three conferences over time.\nConclusion: H12 is supported. Concentration levels differ noticeably between conferences. ICLR appears significantly more ‘elitist’ (higher Top 5 concentration) than ICML and NeurIPS.\n\n\nH13: Institutional Publication Profiles (Clustering)\nHypothesis: Institutions exhibit distinct publication profiles based on conference focus and academia/industry nature, allowing for clustering into meaningful groups.\nQuantification: Calculate features (% NeurIPS, % ICML, % ICLR, % Industry) for institutions with &gt;20 publications. Standardize features and apply K-Means (K=5). Analyze cluster profiles and visualize using PCA.\n\n\nCode\nMIN_PUBS_CLUSTER = 20 # Minimum total publications for an institution to be included\n\nprint(f\"\\nClustering Institutions by Publication Profile (Min {MIN_PUBS_CLUSTER} Pubs)\")\n\n# Filter unknowns\ncluster_df = df[(df['Institution'] != 'Unknown Institution') &\n                (df['Type'].isin(['Academia', 'Industry']))].copy()\n\n# Calculate total publications per institution\ninst_total_counts = cluster_df['Institution'].value_counts()\n\n# Filter institutions below the threshold\ninstitutions_to_keep = inst_total_counts[inst_total_counts &gt;= MIN_PUBS_CLUSTER].index.tolist()\ncluster_df = cluster_df[cluster_df['Institution'].isin(institutions_to_keep)]\n\nif cluster_df.empty:\n    print(f\"No institutions found with at least {MIN_PUBS_CLUSTER} publications.\")\nelse:\n    # --- Feature Calculation ---\n    inst_conf_counts = cluster_df.groupby(['Institution', 'Conference']).size().unstack(fill_value=0)\n    inst_conf_perc = inst_conf_counts.apply(lambda x: x / x.sum() * 100, axis=1)\n    inst_conf_perc.columns = [f'pct_{col.lower()}' for col in inst_conf_perc.columns]\n\n    inst_type_counts = cluster_df.groupby(['Institution', 'Type']).size().unstack(fill_value=0)\n    inst_type_perc = inst_type_counts.apply(lambda x: x / x.sum() * 100, axis=1)\n    if 'Industry' not in inst_type_perc.columns: inst_type_perc['Industry'] = 0\n    if 'Academia' not in inst_type_perc.columns: inst_type_perc['Academia'] = 0\n    inst_type_perc = inst_type_perc[['Industry']].rename(columns={'Industry': 'pct_industry'})\n\n    feature_df = pd.merge(inst_conf_perc, inst_type_perc, left_index=True, right_index=True)\n    expected_feature_cols = ['pct_neurips', 'pct_icml', 'pct_iclr', 'pct_industry']\n    for col in expected_feature_cols:\n         if col not in feature_df.columns: feature_df[col] = 0\n    feature_cols_to_use = [col for col in expected_feature_cols if col in feature_df.columns]\n\n    # --- Clustering ---\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(feature_df[feature_cols_to_use])\n    K_CLUSTERS = 5\n    print(f\"K-Means with K={K_CLUSTERS}\")\n    kmeans = KMeans(n_clusters=K_CLUSTERS, random_state=42, n_init=10)\n    feature_df['Cluster'] = kmeans.fit_predict(features_scaled)\n\n    # --- Analysis & Visualization ---\n    cluster_centers_scaled = kmeans.cluster_centers_\n    cluster_centers = scaler.inverse_transform(cluster_centers_scaled)\n    profile_cols_exist = [col for col in feature_cols_to_use] # Get features used\n    cluster_profile = pd.DataFrame(cluster_centers, columns=profile_cols_exist)\n    print(\"\\nCluster Profiles (Average %):\")\n    print(cluster_profile.round(1))\n\n    # --- PCA Visualization ---\n    print(\"PCA for visualization\")\n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(features_scaled)\n    feature_df['PCA1'] = pca_result[:, 0]\n    feature_df['PCA2'] = pca_result[:, 1]\n    feature_df_reset = feature_df.reset_index()\n\n    fig_h18_pca = px.scatter(feature_df_reset,\n                         x='PCA1', y='PCA2', color='Cluster', # Use integer Cluster for color mapping\n                         color_continuous_scale=px.colors.qualitative.Vivid, # Specify qualitative scale\n                         hover_data={'Institution': True, 'pct_neurips': ':.1f', 'pct_icml': ':.1f',\n                                     'pct_iclr': ':.1f', 'pct_industry': ':.1f', 'Cluster': True},\n                         title=f'Institution Clusters (K={K_CLUSTERS}) based on Publication Profile (PCA Visualization)',\n                         labels={'Cluster': 'Cluster ID'})\n    fig_h18_pca.update_layout(coloraxis_showscale=False) # Hide continuous bar for qualitative scale\n    fig_h18_pca.show()\n\n    # --- Parallel Coordinates Plot ---\n    # Prepare data: reset index of the average profiles, rename 'index' to 'Cluster'\n    plot_data_profiles = cluster_profile.reset_index()\n    plot_data_profiles = plot_data_profiles.rename(columns={'index': 'Cluster'})\n\n    # *** KEEP 'Cluster' AS NUMERICAL (0-4) FOR COLOR MAPPING ***\n    # plot_data_profiles['Cluster'] = plot_data_profiles['Cluster'].astype(str) # REMOVED THIS CONVERSION\n\n    print(\"\\nCluster Profile Comparison Plot\")\n    fig_profiles = px.parallel_coordinates(\n        plot_data_profiles,\n        dimensions=feature_cols_to_use,        # Use only the feature columns for axes\n        color='Cluster',                       # Use the NUMERICAL Cluster ID for color mapping\n        color_continuous_scale=px.colors.qualitative.Vivid, # *** APPLY QUALITATIVE SCALE ***\n        title='Average Publication Profiles by Cluster',\n        labels={col: col.replace('pct_', '% ').replace('_', ' ').title() for col in feature_cols_to_use}\n    )\n    # Optionally hide the color scale bar as it represents discrete categories\n    fig_profiles.update_layout(coloraxis_showscale=False)\n    fig_profiles.show()\n\n\n\nClustering Institutions by Publication Profile (Min 20 Pubs)\nK-Means with K=5\n\nCluster Profiles (Average %):\n   pct_neurips  pct_icml  pct_iclr  pct_industry\n0         83.2      10.5       6.3          16.4\n1         60.2      23.1      16.6           0.1\n2         59.7      24.8      15.4          98.6\n3         42.2      16.9      40.9          43.3\n4         45.2      41.7      13.2           6.8\nPCA for visualization\n\n\n                            \n                                            \n\n\n\nCluster Profile Comparison Plot\n\n\n                            \n                                            \n\n\nFindings: The K-Means clustering (with \\(K=5\\)) identifies distinct groups visualized via PCA and characterized by their average profiles:\n\nCluster 2 (Dark Green on PCA): Industry Giants. This cluster is characterized by an extremely high average pct_industry (98.6%) and a relatively balanced distribution across the three conferences (~60% NeurIPS, ~25% ICML, ~15% ICLR). Sample Institutions likely include Google, Microsoft, Meta, etc.\nCluster 1 (Light Green on PCA): Balanced Academics. This large cluster has virtually 0% industry affiliation and a fairly balanced conference profile, slightly favoring NeurIPS (~60%) over ICML (~23%) and ICLR (~17%). This likely represents many traditional university CS/AI departments.\nCluster 4 (Grey on PCA): ICML-Leaning Academics. Very similar to Cluster 1 (low industry %), but with a significantly higher average focus on ICML (~42%) and lower NeurIPS focus (~45%) compared to Cluster 1.\nCluster 0 (Orange on PCA): NeurIPS-Dominant Academics. This cluster also has low average industry presence (~16%) but shows a strong preference for NeurIPS (~83%) compared to ICML and ICLR (both ~10% or less).\nCluster 3 (Blue on PCA): ICLR-Heavy/Mixed. This is an interesting cluster with a moderate industry presence (~43%) but a very strong focus on ICLR (~41%), much higher than any other cluster, while having lower NeurIPS presence (~42%).\n\nConclusion: H13 is supported. Institutions exhibit distinct publication profiles, allowing K-Means to identify meaningful clusters such as industry powerhouses, balanced academic centers, and institutions with specific conference preferences (NeurIPS-dominant, ICML-leaning, ICLR-heavy)."
  },
  {
    "objectID": "posts/ml-publication-trends/ml_publication_trends.html#discussion",
    "href": "posts/ml-publication-trends/ml_publication_trends.html#discussion",
    "title": "ML Publication Trends",
    "section": "6. Discussion",
    "text": "6. Discussion\nThis analysis of publication trends at NeurIPS, ICML, and ICLR from 2006-2024 reveals several key insights into the evolution of the AI research landscape:\n\nExplosive Growth: The sheer increase in paper volume underscores the field’s rapid expansion and the growing importance of these conferences.\nGeopolitical Shifts: The dramatic rise of China as a leading contributor, challenging the long-standing dominance of the USA, is arguably the most significant geographic trend. Other regions like Europe maintain a strong presence, while countries like South Korea and India are growing their share.\nAcademia-Industry Dynamics: Industry’s role has transitioned from marginal to substantial, accounting for roughly a third of contributions recently. Collaboration between the two sectors has also increased significantly, reflecting the tightening link between fundamental research and industrial application.\nConcentration and Elitism: Research remains heavily concentrated in a relatively small number of globally recognized institutions (both academic and industrial). While there is persistence among the top tier, significant dynamism exists, with new players (e.g., Tsinghua, Peking, Amazon, Mila) rising into the elite ranks in recent years.\nGlobalization: The increasing trend of international co-authorship suggests a more interconnected global research community.\n\nLimitations:\n\nData Scope: The analysis is limited to three major conferences; trends might differ in other venues or subfields. The absence of ICLR 2020 data creates a small gap.\nAffiliation Parsing: While the LLM approach was effective, potential errors in institution standardization, country inference, or type classification remain (~2-5% unmapped/unknown). A dedicated post-processing step with fuzzy matching could improve accuracy further.\nAuthorship vs. Impact: This analysis measures publication volume (authorship instances), not necessarily research impact (e.g., citations, influence). Dominance in publications doesn’t directly equate to dominance in groundbreaking research.\n\n\nDiscussion on the ~2015 Dip/Slowdown\nSeveral plots, including collaboration trends (H6, H7) show a noticeable dip, slowdown, or inflection point around 2015-2016. Potential reasons for this observation include:\n\nNeurIPS Format Change/Lag: NeurIPS (a major contributor to the volume) underwent significant changes around this time. It moved its submission deadline much earlier in 2015 (to May from ~October) for the December conference. This might have disrupted submission patterns for that cycle, potentially causing a temporary dip or displacing some submissions to other venues or the following year. The dataset’s start year for ICML (2017) and ICLR (2018, excl. 2020) means the earlier years are dominated by NeurIPS, making NeurIPS-specific fluctuations more apparent in the aggregate plots before 2017.\nRise of Other Venues: While NeurIPS, ICML, and ICLR are top-tier, the rise of other specialized conferences or workshops around this period might have drawn some papers away, causing a temporary slowdown in the growth rate at these specific venues.\n“Deep Learning Winter” Echoes / Transition Period?: While the major AI winter was much earlier, 2014-2016 was a period of intense consolidation around deep learning methods after initial breakthroughs (e.g., AlexNet 2012). It’s conceivable there was a brief pause or shift in focus before the next wave of major advancements (like attention/transformers gaining traction post-2017) fully accelerated publication rates again.\nData Artifacts: Although less likely with publication counts, subtle data collection inconsistencies or changes in how conferences reported papers online during that specific year could theoretically contribute.\nSaturation/Reviewer Load: Rapid growth before 2015 might have started straining the peer review system, potentially leading to slightly lower acceptance rates or slower review cycles temporarily affecting the number of published papers for a cycle.\n\nWithout more specific context on conference policies or submission numbers for those exact years, it’s hard to pinpoint a single cause. However, the NeurIPS deadline shift and the general dynamics of a rapidly evolving field transitioning between major paradigms seem like plausible contributing factors."
  },
  {
    "objectID": "posts/ml-publication-trends/ml_publication_trends.html#conclusion",
    "href": "posts/ml-publication-trends/ml_publication_trends.html#conclusion",
    "title": "ML Publication Trends",
    "section": "7. Conclusion",
    "text": "7. Conclusion\nThe AI research landscape, as viewed through the lens of NeurIPS, ICML, and ICLR publications, is characterized by rapid growth, significant geographic power shifts (especially the rise of China), increasing industry involvement and collaboration, and a persistent but evolving concentration of research within elite global institutions. The use of Large Language Models proved crucial for extracting the necessary structured data from complex affiliation strings, enabling these quantitative insights. Future work could involve expanding the conference scope, refining affiliation normalization, and incorporating citation data for impact analysis."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]
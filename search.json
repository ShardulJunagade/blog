[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBuilding a Solar Panel Detector\n\n\n\n\n\n\ncode\n\n\nobject detection\n\n\ncomputer vision\n\n\n\nTraining a solar panel detector using YOLOv12 to detect solar panels in aerial images.\n\n\n\n\n\nFeb 27, 2025\n\n\nShardul Junagade\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nShardul Junagade\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html",
    "href": "posts/solar_panel_detector/solar_panel_detector.html",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "In this notebook, we will build a solar panel detector that can detect solar panels in aerial images. We’ll use the YOLOv12 model, which is the latest state-of-the-art object detection model from Ultralytics, to identify and locate solar panels in high-resolution aerial imagery.\nThis project will demonstrate:\n\nData preprocessing and exploration of the Solar Panel Object Labels dataset\nImplementation of evaluation metrics (IoU, AP)\nTraining and fine-tuning of the YOLOv12 model\nEvaluation of model performance on test data\nVisualization and interpretation of results\n\n\n\n\n\nCode\nimport torch\nfrom ultralytics import YOLO\nimport supervision as sv\nimport shapely.geometry as sg\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n\nUsing device: cuda\n\n\n\n\n\nWe will use the Solar Panel Object Labels dataset from Figshare. This dataset contains high-resolution aerial images with labeled solar panels. We will use the 31 cm native resolution images of sizes 416x416 pixels for our analysis.\nThe dataset files follow a specific naming structure: solarpanels_native_&lt;image_number&gt;__x0_&lt;x_coordinate&gt;_y0_&lt;y_coordinate&gt;_dxdy_&lt;size&gt;.\nFor example, in the file solarpanels_native_1__x0_0_y0_6845_dxdy_416.txt:\n\nsolarpanels_native: Indicates that the file contains solar panel data in native resolution.\n&lt;image_number&gt;: The number of the image in the dataset.\nx0_&lt;x_coordinate&gt;: The x-coordinate of the top-left corner of the image.\ny0_&lt;y_coordinate&gt;: The y-coordinate of the top-left corner of the image.\ndxdy_&lt;size&gt;: The size of the image in pixels (both width and height).\n\nEach line in the file represents a detected solar panel with the following format: category x_center y_center width height, where:\n\ncategory: The category label (0 for solar panels).\nx_center: The x-coordinate of the center of the bounding box (normalized).\ny_center: The y-coordinate of the center of the bounding box (normalized).\nwidth: The width of the bounding box (normalized).\nheight: The height of the bounding box (normalized).\n\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2553\nNumber of labels: 2542\n\n\nThe number of images and the number of labels is not the same. So, we can remove the images that do not have corresponding labels and remove the labels that do not have corresponding images.\n\n\nCode\n# delete the images that do not have corresponding labels\nfor image_name in image_names:\n    if image_name.replace('.tif', '.txt') not in label_names:\n        os.remove(image_dir + image_name)\n\n# delete the labels that do not have corresponding images\nfor label_name in label_names:\n    if label_name.replace('.txt', '.tif') not in image_names:\n        os.remove(label_dir + label_name)\n\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2542\nNumber of labels: 2542\n\n\n\n\n\n\nCode\ntotal_instances = 0\nclass_count = {}\n\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        total_instances += len(lines)\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\n\nclass_count = dict(sorted(class_count.items()))\n\nprint('Total instances:', total_instances)\nprint('\\nNumber of unique classes:', len(class_count))\nprint('\\nClass-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nTotal instances: 29625\n\nNumber of unique classes: 3\n\nClass-wise distribution:\n    Class 0: 29267\n    Class 1: 130\n    Class 2: 228\n\n\nSince, we are doing a detection task, I converted all labels of class 1 and 2 to class 0.\n\n\nCode\n# convert all classes to 0 in the labels\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    with open(label_path, 'w') as f:\n        for line in lines:\n            f.write('0 ' + ' '.join(line.split()[1:]) + '\\n')\n\n# updated class count\nclass_count = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\nprint('Updated class-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nUpdated class-wise distribution:\n    Class 0: 29625\n\n\n\n\nCode\n# Calculate number of images having a particular number of labels\nlabel_distribution = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        num_labels = len(lines)\n        label_distribution[num_labels] = label_distribution.get(num_labels, 0) + 1\n\nlabel_distribution = dict(sorted(label_distribution.items()))\nprint('Value counts of labels per image:')\nfor num_labels, count in label_distribution.items():\n    print(f'{count} images have {num_labels} labels.')\n\n\nValue counts of labels per image:\n81 images have 1 labels.\n167 images have 2 labels.\n221 images have 3 labels.\n218 images have 4 labels.\n217 images have 5 labels.\n189 images have 6 labels.\n170 images have 7 labels.\n184 images have 8 labels.\n169 images have 9 labels.\n121 images have 10 labels.\n97 images have 11 labels.\n84 images have 12 labels.\n69 images have 13 labels.\n49 images have 14 labels.\n46 images have 15 labels.\n41 images have 16 labels.\n36 images have 17 labels.\n25 images have 18 labels.\n29 images have 19 labels.\n14 images have 20 labels.\n4 images have 21 labels.\n1 images have 22 labels.\n4 images have 23 labels.\n2 images have 24 labels.\n4 images have 25 labels.\n3 images have 26 labels.\n5 images have 27 labels.\n5 images have 28 labels.\n15 images have 29 labels.\n20 images have 30 labels.\n8 images have 31 labels.\n7 images have 32 labels.\n13 images have 33 labels.\n19 images have 34 labels.\n10 images have 35 labels.\n6 images have 36 labels.\n17 images have 37 labels.\n13 images have 38 labels.\n6 images have 39 labels.\n9 images have 40 labels.\n10 images have 41 labels.\n12 images have 42 labels.\n11 images have 43 labels.\n4 images have 44 labels.\n2 images have 45 labels.\n5 images have 46 labels.\n9 images have 47 labels.\n3 images have 48 labels.\n5 images have 49 labels.\n6 images have 50 labels.\n9 images have 51 labels.\n16 images have 52 labels.\n4 images have 53 labels.\n6 images have 54 labels.\n1 images have 55 labels.\n1 images have 56 labels.\n3 images have 58 labels.\n2 images have 59 labels.\n2 images have 60 labels.\n1 images have 61 labels.\n6 images have 62 labels.\n3 images have 63 labels.\n1 images have 64 labels.\n3 images have 65 labels.\n4 images have 66 labels.\n1 images have 67 labels.\n1 images have 71 labels.\n1 images have 72 labels.\n1 images have 73 labels.\n5 images have 74 labels.\n1 images have 75 labels.\n2 images have 76 labels.\n2 images have 77 labels.\n1 images have 78 labels.\n\n\n\n\n\nWe can calculate the area of the solar panels (in square meters) as follows:\n\nDenormalize x-width and y-width:\n\nMultiply by the chip size:\n\nNative resolution (31 cm): \\(416 \\times 416\\)\nHD resolution (15.5 cm): \\(832 \\times 832\\)\n\n\nConvert to real-world meters:\n\nPixel size = 0.31 meters per pixel\nReal width & height in meters: \\[\n\\text{real\\_width} = x\\_width \\times 416 \\times 0.31\n\\] \\[\n\\text{real\\_height} = y\\_width \\times 416 \\times 0.31\n\\]\n\nCompute area: \\[\n\\text{area} = \\text{real\\_width} \\times \\text{real\\_height}\n\\]\n\n\n\nCode\nareas = []\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name, x_center, y_center, width, height = map(float, line.split())\n            # print(class_name, x_center, y_center, width, height)\n            real_width = width * image_size * meters_per_pixel\n            real_height = height * image_size * meters_per_pixel\n            area = real_width * real_height\n            areas.append(area)\n\nareas = np.array(areas)\nmean_area = np.mean(areas)\nstd_area = np.std(areas)\nprint(f'Mean area of solar panels: {mean_area:.2f} m^2')\nprint(f'Standard deviation of area of solar panels: {std_area:.2f} m^2')\n\nplt.figure(figsize=(8, 6))\nplt.hist(areas, bins=50, color='blue', edgecolor='black', alpha=0.7)\nplt.xlabel('Area (m^2)')\nplt.ylabel('Frequency')\nplt.title('Histogram of areas of solar panels')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\nMean area of solar panels: 191.52 m^2\nStandard deviation of area of solar panels: 630.70 m^2\n\n\n\n\n\n\n\n\n\nFrom the above histogram, we can observe the following:\n\nThe majority of the solar panels have areas concentrated around the lower end of the scale.\nThere are fewer instances of solar panels with larger areas.\nThe distribution appears to be right-skewed, indicating that most solar panels are relatively small in size, with a few larger ones.\n\n\n\n\n\n\n\nIntersection over Union (IoU) is a metric used to evaluate the accuracy of an object detector on a particular dataset. It measures the overlap between two bounding boxes: the predicted bounding box and the ground truth bounding box.\nThe IoU is calculated as follows:\n\nIntersection: The area of overlap between the predicted bounding box and the ground truth bounding box.\nUnion: The total area covered by both the predicted bounding box and the ground truth bounding box.\n\nThe IoU is then computed as the ratio of the intersection area to the union area:\n\\[\n\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n\\]\nThe IoU value ranges from 0 to 1, where:\n\n0 indicates no overlap between the bounding boxes.\n1 indicates a perfect overlap between the bounding boxes.\n\nUsage of Shapely Library:\nShapely’s Polygon class is used to represent bounding boxes as geometric shapes, defined by their corner coordinates. The intersection area between two polygons is computed using the .intersection() method, which finds the overlapping region of the two bounding boxes. The union area is determined using the .union() method, which combines both polygons into a single shape.\n\n\nCode\n# convert yolo format to x_min, y_min, x_max, y_max format\ndef yolo_to_xyxy(yolo_bbox, image_size):\n    if len(yolo_bbox) == 5:\n        class_id, x_center, y_center, width, height = yolo_bbox\n    else:\n        x_center, y_center, width, height = yolo_bbox\n    x_min = (x_center - width / 2) * image_size\n    y_min = (y_center - height / 2) * image_size\n    x_max = (x_center + width / 2) * image_size\n    y_max = (y_center + height / 2) * image_size\n    return x_min, y_min, x_max, y_max\n\n# calculate iou using shapely\ndef iou_shapely(yolo_bbox1, yolo_bbox2, image_size):\n    x_min1, y_min1, x_max1, y_max1 = yolo_to_xyxy(yolo_bbox1, image_size)\n    x_min2, y_min2, x_max2, y_max2 = yolo_to_xyxy(yolo_bbox2, image_size)\n\n    polygon1 = sg.Polygon([(x_min1, y_min1), (x_max1, y_min1), (x_max1, y_max1), (x_min1, y_max1)])\n    polygon2 = sg.Polygon([(x_min2, y_min2), (x_max2, y_min2), (x_max2, y_max2), (x_min2, y_max2)])\n\n    intersection_area = polygon1.intersection(polygon2).area\n    union_area = polygon1.union(polygon2).area\n\n    if union_area == 0.0:\n        iou = 0.0\n    else:\n        iou = intersection_area / union_area\n    return iou\n\n# calculate iou using supervision\ndef iou_supervision(yolo_bbox1, yolo_bbox2, image_size):\n    box1 = np.array([yolo_to_xyxy(yolo_bbox1, image_size)])\n    box2 = np.array([yolo_to_xyxy(yolo_bbox2, image_size)])\n    iou_matrix = sv.box_iou_batch(box1, box2)\n    return iou_matrix[0, 0]\n\n\n\n\nCode\n# Example usage on YOLO bounding boxes (class_id, x_center, y_center, width, height) \nyolo_bbox1 = [0, 0.1, 0.1, 0.6, 0.6]\nyolo_bbox2 = [0, 0.3, 0.3, 0.6, 0.6]\n\niou1 = iou_shapely(yolo_bbox1, yolo_bbox2, image_size)\niou2 = iou_supervision(yolo_bbox1, yolo_bbox2, image_size)\n\nprint(f'IoU computed using shapely: {iou1:.4f}')\nprint(f'IoU computed using supervision: {iou2:.4f}')\n\n\nIoU computed using shapely: 0.2857\nIoU computed using supervision: 0.2857\n\n\n\n\n\nAverage Precision (AP) is a metric used to evaluate the performance of object detection models. It summarizes the precision-recall curve into a single value, representing the average of precision values at different recall levels.\n\nPrecision: The ratio of true positive detections to the total number of positive detections (true positives + false positives).\n\\[\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\nRecall: The ratio of true positive detections to the total number of actual positives (true positives + false negatives).\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\n\nThe precision-recall curve is plotted with precision on the y-axis and recall on the x-axis. The Average Precision (AP) is calculated as the area under the precision-recall curve.\nThere are different methods to compute AP:\n\nPascal VOC 11-point interpolation: Precision is sampled at 11 recall levels (0.0, 0.1, …, 1.0), and the average of these precision values is taken.\nCOCO 101-point interpolation: Precision is sampled at 101 recall levels (0.0, 0.01, …, 1.0), and the average of these precision values is taken.\nArea under Precision-Recall Curve (PRC): The area under the precision-recall curve is computed using numerical integration methods.\n\n\n\nCode\nfrom sklearn.metrics import auc\n\ndef compute_ap(precisions, recalls, method=\"voc11\"):\n    # Pascal VOC 11-point method\n    if method == \"voc11\":\n        recall_points = np.linspace(0, 1, 11)\n        ap = 0.0\n        for r in recall_points:\n            ap += max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0\n        ap /= 11\n    # COCO 101-point method\n    elif method == \"coco101\":\n        recall_points = np.linspace(0, 1, 101)\n        ap = np.mean([max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0 for r in recall_points])\n    # AUC method\n    elif method == \"auc_pr\":\n        ap = auc(recalls, precisions)\n    else:\n        raise ValueError(\"Invalid AP computation method.\")\n    return ap\n\n\n# calculate precision and recall values\ndef compute_precision_recall(gt_boxes, pred_boxes, scores, image_size, iou_threshold=0.5):\n    all_tp = []\n    all_fp = []\n    num_gt_boxes = sum(len(gt) for gt in gt_boxes)\n    if num_gt_boxes == 0:\n        return np.array([]), np.array([])\n\n    for gt_boxes_image, pred_boxes_image, scores_image in zip(gt_boxes, pred_boxes, scores):\n        # Sort predicted boxes by confidence score in descending order\n        sorted_indices = np.argsort(scores_image)[::-1]\n        pred_boxes_image = [pred_boxes_image[idx] for idx in sorted_indices]\n        scores_image = [scores_image[idx] for idx in sorted_indices]\n\n        tp = np.zeros(len(pred_boxes_image))\n        fp = np.zeros(len(pred_boxes_image))\n        gt_matched = np.zeros(len(gt_boxes_image))\n\n        for j, pred_box in enumerate(pred_boxes_image):\n            if len(gt_boxes_image) == 0:\n                fp[j] = 1\n                continue\n            \n            ious = [iou_shapely(pred_box, gt_box, image_size) for gt_box in gt_boxes_image]\n            max_iou = max(ious) if ious else 0\n            max_iou_idx = np.argmax(ious) if ious else -1\n\n            if max_iou &gt;= iou_threshold and max_iou_idx != -1 and gt_matched[max_iou_idx] == 0:\n                tp[j] = 1\n                gt_matched[max_iou_idx] = 1\n            else:\n                fp[j] = 1\n\n        all_tp.extend(tp)\n        all_fp.extend(fp)\n\n    # Compute Precision-Recall\n    tp_cumsum = np.cumsum(all_tp)\n    fp_cumsum = np.cumsum(all_fp)\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)  # Avoid division by zero\n    recalls = tp_cumsum / num_gt_boxes\n\n    return precisions, recalls\n\n\n\n\n\n\n\nCode\n# generate random data\ndef generate_random_data(num_images, image_size, num_gt_boxes, num_pred_boxes, box_size):\n    images = []\n    gt_boxes = []\n    pred_boxes = []\n    scores = []\n\n    for _ in range(num_images):\n        image = np.random.rand(image_size, image_size)\n        images.append(image)\n\n        gt_boxes_image = []\n        for _ in range(num_gt_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            gt_boxes_image.append([x_min, y_min, x_max, y_max])\n        gt_boxes.append(gt_boxes_image)\n\n        pred_boxes_image = []\n        scores_image = []\n        for _ in range(num_pred_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            pred_boxes_image.append([x_min, y_min, x_max, y_max])\n            scores_image.append(np.random.uniform(0.5, 1.0))\n        pred_boxes.append(pred_boxes_image)\n        scores.append(scores_image)\n\n    return images, gt_boxes, pred_boxes, scores\n\n\n\n\nCode\nrandom_num_images = 10\nrandom_image_size = 100\nrandom_num_gt_boxes = 10\nrandom_num_pred_boxes = 10\nrandom_box_size = 20\nrandom_images, random_gt_boxes, random_pred_boxes, random_scores = generate_random_data(random_num_images, random_image_size, random_num_gt_boxes, random_num_pred_boxes, random_box_size)\n\n\n\n\nCode\nprecisions, recalls = compute_precision_recall(random_gt_boxes, random_pred_boxes, random_scores, random_image_size, iou_threshold=0.5)\n\n# plot precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recalls, precisions, color='blue', linestyle='-', linewidth=2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute AP50\nap50_voc11 = compute_ap(precisions, recalls, method=\"voc11\")\nap50_coco101 = compute_ap(precisions, recalls, method=\"coco101\")\nap50_auc_pr = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f'AP50 using VOC11 method: {ap50_voc11:.4f}')\nprint(f'AP50 using COCO101 method: {ap50_coco101:.4f}')\nprint(f'AP50 using AUC-PR method: {ap50_auc_pr:.4f}')\n\n\nAP50 using VOC11 method: 0.2685\nAP50 using COCO101 method: 0.2571\nAP50 using AUC-PR method: 0.2188\n\n\n\n\n\n\n\n\nWe will split the data into training and testing sets using an 80-20 split. We will further split the training data into training and validation sets using a 90-10 split. This will allow us to train the model on the training set and tune the hyperparameters on the validation set. The testing set will be used to evaluate the model’s performance on unseen data.\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\ndef split_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1):\n    os.makedirs(save_dir, exist_ok=True)\n    for split in [\"train\", \"val\", \"test\"]:\n        os.makedirs(os.path.join(save_dir, split, \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(save_dir, split, \"labels\"), exist_ok=True)\n    \n    image_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\n    label_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n    num_images = len(image_names)\n\n    # shuffle the data\n    indices = np.arange(num_images)\n    np.random.shuffle(indices)\n\n    train_size = int(train_ratio * num_images)\n    val_size = int(val_ratio * train_size)\n    test_size = num_images - train_size\n\n    train_indices = indices[:train_size]\n    val_indices = train_indices[-val_size:]\n    train_indices = train_indices[:-val_size]\n    test_indices = indices[train_size:]\n\n    def copy_data(indices, split):\n        for idx in indices:\n            image_src = os.path.join(image_dir, image_names[idx])\n            label_src = os.path.join(label_dir, label_names[idx])\n\n            image_dst = os.path.join(save_dir, split, \"images\", image_names[idx])\n            label_dst = os.path.join(save_dir, split, \"labels\", label_names[idx])\n\n            shutil.copy(image_src, image_dst)\n            shutil.copy(label_src, label_dst)\n\n    copy_data(train_indices, \"train\")\n    copy_data(val_indices, \"val\")\n    copy_data(test_indices, \"test\")\n\n    print(f\"Data split and saved in '{save_dir}/' successfully!\")\n    print(f\"Train:      {len(train_indices)} images\")\n    print(f\"Validation: {len(val_indices)} images\")\n    print(f\"Test:       {len(test_indices)} images\")\n\n\n\nsplit_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1)\n\n\nData split and saved in 'split_data/' successfully!\nTrain:      1830 images\nValidation: 203 images\nTest:       509 images\n\n\n\n\nCode\n# Create YAML content with absolute paths\nbase_path = os.path.abspath('./split_data')\nyaml_content = f\"\"\"path: {base_path}\ntrain: {os.path.join(base_path, 'train', 'images')}\nval: {os.path.join(base_path, 'val', 'images')}\ntest: {os.path.join(base_path, 'test', 'images')}\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\"\"\"\ndata_yml_path = os.path.join(base_path, 'data.yaml')\nwith open(data_yml_path, 'w') as f:\n    f.write(yaml_content)\nprint(f\"data.yaml created at:\\n{data_yml_path}\")\ndel base_path\n\n\ndata.yaml created at:\nc:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\data.yaml\n\n\n\n\nCode\nprint(yaml_content)\n\n\npath: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\ntrain: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\train\\images\nval: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\val\\images\ntest: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\n\n\n\n\n\n\n\nCode\nmodel = YOLO(\"yolo12x.pt\")\n\nresults = model.train(\n    data=\"split_data/data.yaml\",\n    epochs=100,\n    imgsz=416,\n    batch=100,\n    device=device,\n    project=\"models\",\n    name=\"yolo12x\",\n)\n\n\n\n\nCode\n# load the best model\nmodel = YOLO(\"models/yolo12x/weights/best.pt\")\n\n\n\n\nCode\ndf = pd.read_csv('models/yolo12x/results.csv')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# First subplot: Training loss\nax1.plot(df['epoch'], df['train/box_loss'], label='Box Loss')\nax1.plot(df['epoch'], df['train/cls_loss'], label='Cls Loss')\nax1.plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss')\ndf['train/total_loss'] = df['train/box_loss'] + df['train/cls_loss'] + df['train/dfl_loss']\nax1.plot(df['epoch'], df['train/total_loss'], label='Total Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training Loss')\nax1.legend()\nax1.grid(True)\n\n# Second subplot: Validation loss\nax2.plot(df['epoch'], df['val/box_loss'], label='Box Loss')\nax2.plot(df['epoch'], df['val/cls_loss'], label='Cls Loss')\nax2.plot(df['epoch'], df['val/dfl_loss'], label='DFL Loss')\ndf['val/total_loss'] = df['val/box_loss'] + df['val/cls_loss'] + df['val/dfl_loss']\nax2.plot(df['epoch'], df['val/total_loss'], label='Total Loss')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss')\nax2.set_title('Validation Loss')\nax2.legend()\nax2.set_ylim(ax1.get_ylim())\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load the test data\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\n# Randomly select 3-4 images\nnum_images = 4\nrandom_images = np.random.choice(test_images, num_images, replace=False)\n\nfig, axes = plt.subplots(num_images, 2, figsize=(15, 5 * num_images))\n\nfor i, image_name in enumerate(random_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Predict using the trained model\n    results = model(image_path)[0]  # Get the first result object\n\n    # Load the image\n    image = plt.imread(image_path)\n\n    # Plot the ground truth labels (green)\n    with open(label_path, 'r') as f:\n        gt_labels = f.readlines()\n    axes[i, 0].imshow(image)\n    axes[i, 0].axis('off')\n    axes[i, 0].set_title(f\"Ground Truth: {image_name}\")\n    for gt_label in gt_labels:\n        class_id, x_center, y_center, width, height = map(float, gt_label.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='green', facecolor='none')\n        axes[i, 0].add_patch(rect)\n        axes[i, 0].text(x_min, y_min - 5, 'Ground Truth', color='green', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\n    # Plot the predicted labels (red)\n    axes[i, 1].imshow(image)\n    axes[i, 1].axis('off')\n    axes[i, 1].set_title(f\"Predictions: {image_name}\")\n    boxes = results.boxes\n    for box in boxes:\n        x_min, y_min, x_max, y_max = box.xyxy[0].cpu().numpy()\n        conf = float(box.conf[0])\n        cls = int(box.cls[0])\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='blue', facecolor='none')\n        axes[i, 1].add_patch(rect)\n        axes[i, 1].text(x_min, y_max + 15, f'Pred: {conf:.2f}', color='blue', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\n# save figure\nfig.savefig('predictions.png')\n\n\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3870_y0_11610_dxdy_416.tif: 416x416 19 solar_panels, 124.0ms\nSpeed: 2.3ms preprocess, 124.0ms inference, 4.5ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3061_y0_8047_dxdy_416.tif: 416x416 5 solar_panels, 84.0ms\nSpeed: 2.5ms preprocess, 84.0ms inference, 4.9ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_7463_y0_9890_dxdy_416.tif: 416x416 8 solar_panels, 91.4ms\nSpeed: 1.7ms preprocess, 91.4ms inference, 4.3ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_8067_y0_10432_dxdy_416.tif: 416x416 2 solar_panels, 69.2ms\nSpeed: 1.6ms preprocess, 69.2ms inference, 4.6ms postprocess per image at shape (1, 3, 416, 416)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef load_detections(image_path):\n    label_path = image_path.replace('images', 'labels').replace('.tif', '.txt')\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    \n    xyxy_list = []\n    class_ids = []\n    scores = []\n\n    for line in lines:\n        class_id, x_center, y_center, width, height = map(float, line.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        xyxy_list.append([x_min, y_min, x_max, y_max])\n        class_ids.append(class_id)\n        scores.append(1.0)\n\n    detections = sv.Detections(\n        xyxy=np.array(xyxy_list),\n        class_id=np.array(class_ids),\n        confidence=np.array(scores),\n        metadata={\"image_name\": os.path.basename(image_path)}\n    )\n\n    return detections\n\n\n\n\nCode\n# load the ground truth and predictions\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\ntargets = []\npredictions = []\n\nfor i, image_name in enumerate(test_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Load the ground truth and predictions\n    target_sv_detection = load_detections(image_path)\n    # print(target_sv_detection.xyxy)\n\n    results = model(image_path, verbose=False)[0] \n    pred_sv_detection = sv.Detections.from_ultralytics(results)\n    # print(pred_sv_detection.xyxy)\n\n    targets.append(target_sv_detection)\n    predictions.append(pred_sv_detection)\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\nimport pickle\n\n# Save the detections\nwith open(\"detections.pkl\", \"wb\") as f:\n    pickle.dump({\"targets\": targets, \"predictions\": predictions}, f)\n\n# Load the detections\nwith open(\"detections.pkl\", \"rb\") as f:\n    data = pickle.load(f)\ntargets = data[\"targets\"]\npredictions = data[\"predictions\"]\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\n# Compute mAP50 using supervision\nmetrics = sv.metrics.MeanAveragePrecision()\nresults = metrics.update(targets, predictions).compute()\nmAP50_supervision = results.map50\nprint(f\"mAP50 using supervision: {mAP50_supervision:.4f}\")\n\n\nmAP50 using supervision: 0.9018\n\n\n\n\nCode\n# Compute mAP50 using my implementation\nall_gt_boxes, all_pred_boxes, all_scores = [], [], []\n\nfor target, prediction in zip(targets, predictions):\n    all_gt_boxes.append(target.xyxy)\n    all_pred_boxes.append(prediction.xyxy)\n    all_scores.append(prediction.confidence)\n\nprecisions, recalls = compute_precision_recall(all_gt_boxes, all_pred_boxes, all_scores, image_size, iou_threshold=0.5)\nmAP50_mine = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f\"mAP50 using my implementation: {mAP50_mine:.4f}\")\n\n\nmAP50 using my implementation: 0.8930\n\n\nTODO:\nCreate a table of Precision, Recall and F1-scores where rows are IoU thresholds [0.1, 0.3, 0.5, 0.7, 0.9] and columns are confidence thresholds [0.1, 0.3, 0.5, 0.7, 0.9]\n(Hint use supervision.metrics.ConfusionMatrix to get the confusion matrix and get TP, FP and FN from it to compute the P, R and F-1)"
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html#data-exploration-and-understanding",
    "href": "posts/solar_panel_detector/solar_panel_detector.html#data-exploration-and-understanding",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "We will use the Solar Panel Object Labels dataset from Figshare. This dataset contains high-resolution aerial images with labeled solar panels. We will use the 31 cm native resolution images of sizes 416x416 pixels for our analysis.\nThe dataset files follow a specific naming structure: solarpanels_native_&lt;image_number&gt;__x0_&lt;x_coordinate&gt;_y0_&lt;y_coordinate&gt;_dxdy_&lt;size&gt;.\nFor example, in the file solarpanels_native_1__x0_0_y0_6845_dxdy_416.txt:\n\nsolarpanels_native: Indicates that the file contains solar panel data in native resolution.\n&lt;image_number&gt;: The number of the image in the dataset.\nx0_&lt;x_coordinate&gt;: The x-coordinate of the top-left corner of the image.\ny0_&lt;y_coordinate&gt;: The y-coordinate of the top-left corner of the image.\ndxdy_&lt;size&gt;: The size of the image in pixels (both width and height).\n\nEach line in the file represents a detected solar panel with the following format: category x_center y_center width height, where:\n\ncategory: The category label (0 for solar panels).\nx_center: The x-coordinate of the center of the bounding box (normalized).\ny_center: The y-coordinate of the center of the bounding box (normalized).\nwidth: The width of the bounding box (normalized).\nheight: The height of the bounding box (normalized).\n\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2553\nNumber of labels: 2542\n\n\nThe number of images and the number of labels is not the same. So, we can remove the images that do not have corresponding labels and remove the labels that do not have corresponding images.\n\n\nCode\n# delete the images that do not have corresponding labels\nfor image_name in image_names:\n    if image_name.replace('.tif', '.txt') not in label_names:\n        os.remove(image_dir + image_name)\n\n# delete the labels that do not have corresponding images\nfor label_name in label_names:\n    if label_name.replace('.txt', '.tif') not in image_names:\n        os.remove(label_dir + label_name)\n\nimage_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\nlabel_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n\nprint('Number of images:', len(image_names))\nprint('Number of labels:', len(label_names))\n\n\nNumber of images: 2542\nNumber of labels: 2542\n\n\n\n\n\n\nCode\ntotal_instances = 0\nclass_count = {}\n\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        total_instances += len(lines)\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\n\nclass_count = dict(sorted(class_count.items()))\n\nprint('Total instances:', total_instances)\nprint('\\nNumber of unique classes:', len(class_count))\nprint('\\nClass-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nTotal instances: 29625\n\nNumber of unique classes: 3\n\nClass-wise distribution:\n    Class 0: 29267\n    Class 1: 130\n    Class 2: 228\n\n\nSince, we are doing a detection task, I converted all labels of class 1 and 2 to class 0.\n\n\nCode\n# convert all classes to 0 in the labels\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    with open(label_path, 'w') as f:\n        for line in lines:\n            f.write('0 ' + ' '.join(line.split()[1:]) + '\\n')\n\n# updated class count\nclass_count = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name = line.split()[0]\n            class_count[class_name] = class_count.get(class_name, 0) + 1\nprint('Updated class-wise distribution:')\nfor class_name, count in class_count.items():\n    print(f'    Class {class_name}: {count}')\n\n\nUpdated class-wise distribution:\n    Class 0: 29625\n\n\n\n\nCode\n# Calculate number of images having a particular number of labels\nlabel_distribution = {}\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        num_labels = len(lines)\n        label_distribution[num_labels] = label_distribution.get(num_labels, 0) + 1\n\nlabel_distribution = dict(sorted(label_distribution.items()))\nprint('Value counts of labels per image:')\nfor num_labels, count in label_distribution.items():\n    print(f'{count} images have {num_labels} labels.')\n\n\nValue counts of labels per image:\n81 images have 1 labels.\n167 images have 2 labels.\n221 images have 3 labels.\n218 images have 4 labels.\n217 images have 5 labels.\n189 images have 6 labels.\n170 images have 7 labels.\n184 images have 8 labels.\n169 images have 9 labels.\n121 images have 10 labels.\n97 images have 11 labels.\n84 images have 12 labels.\n69 images have 13 labels.\n49 images have 14 labels.\n46 images have 15 labels.\n41 images have 16 labels.\n36 images have 17 labels.\n25 images have 18 labels.\n29 images have 19 labels.\n14 images have 20 labels.\n4 images have 21 labels.\n1 images have 22 labels.\n4 images have 23 labels.\n2 images have 24 labels.\n4 images have 25 labels.\n3 images have 26 labels.\n5 images have 27 labels.\n5 images have 28 labels.\n15 images have 29 labels.\n20 images have 30 labels.\n8 images have 31 labels.\n7 images have 32 labels.\n13 images have 33 labels.\n19 images have 34 labels.\n10 images have 35 labels.\n6 images have 36 labels.\n17 images have 37 labels.\n13 images have 38 labels.\n6 images have 39 labels.\n9 images have 40 labels.\n10 images have 41 labels.\n12 images have 42 labels.\n11 images have 43 labels.\n4 images have 44 labels.\n2 images have 45 labels.\n5 images have 46 labels.\n9 images have 47 labels.\n3 images have 48 labels.\n5 images have 49 labels.\n6 images have 50 labels.\n9 images have 51 labels.\n16 images have 52 labels.\n4 images have 53 labels.\n6 images have 54 labels.\n1 images have 55 labels.\n1 images have 56 labels.\n3 images have 58 labels.\n2 images have 59 labels.\n2 images have 60 labels.\n1 images have 61 labels.\n6 images have 62 labels.\n3 images have 63 labels.\n1 images have 64 labels.\n3 images have 65 labels.\n4 images have 66 labels.\n1 images have 67 labels.\n1 images have 71 labels.\n1 images have 72 labels.\n1 images have 73 labels.\n5 images have 74 labels.\n1 images have 75 labels.\n2 images have 76 labels.\n2 images have 77 labels.\n1 images have 78 labels.\n\n\n\n\n\nWe can calculate the area of the solar panels (in square meters) as follows:\n\nDenormalize x-width and y-width:\n\nMultiply by the chip size:\n\nNative resolution (31 cm): \\(416 \\times 416\\)\nHD resolution (15.5 cm): \\(832 \\times 832\\)\n\n\nConvert to real-world meters:\n\nPixel size = 0.31 meters per pixel\nReal width & height in meters: \\[\n\\text{real\\_width} = x\\_width \\times 416 \\times 0.31\n\\] \\[\n\\text{real\\_height} = y\\_width \\times 416 \\times 0.31\n\\]\n\nCompute area: \\[\n\\text{area} = \\text{real\\_width} \\times \\text{real\\_height}\n\\]\n\n\n\nCode\nareas = []\nfor label_name in label_names:\n    label_path = os.path.join(label_dir, label_name)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            class_name, x_center, y_center, width, height = map(float, line.split())\n            # print(class_name, x_center, y_center, width, height)\n            real_width = width * image_size * meters_per_pixel\n            real_height = height * image_size * meters_per_pixel\n            area = real_width * real_height\n            areas.append(area)\n\nareas = np.array(areas)\nmean_area = np.mean(areas)\nstd_area = np.std(areas)\nprint(f'Mean area of solar panels: {mean_area:.2f} m^2')\nprint(f'Standard deviation of area of solar panels: {std_area:.2f} m^2')\n\nplt.figure(figsize=(8, 6))\nplt.hist(areas, bins=50, color='blue', edgecolor='black', alpha=0.7)\nplt.xlabel('Area (m^2)')\nplt.ylabel('Frequency')\nplt.title('Histogram of areas of solar panels')\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n\nMean area of solar panels: 191.52 m^2\nStandard deviation of area of solar panels: 630.70 m^2\n\n\n\n\n\n\n\n\n\nFrom the above histogram, we can observe the following:\n\nThe majority of the solar panels have areas concentrated around the lower end of the scale.\nThere are fewer instances of solar panels with larger areas.\nThe distribution appears to be right-skewed, indicating that most solar panels are relatively small in size, with a few larger ones."
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html#implementing-the-fundamental-functions",
    "href": "posts/solar_panel_detector/solar_panel_detector.html#implementing-the-fundamental-functions",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "Intersection over Union (IoU) is a metric used to evaluate the accuracy of an object detector on a particular dataset. It measures the overlap between two bounding boxes: the predicted bounding box and the ground truth bounding box.\nThe IoU is calculated as follows:\n\nIntersection: The area of overlap between the predicted bounding box and the ground truth bounding box.\nUnion: The total area covered by both the predicted bounding box and the ground truth bounding box.\n\nThe IoU is then computed as the ratio of the intersection area to the union area:\n\\[\n\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\n\\]\nThe IoU value ranges from 0 to 1, where:\n\n0 indicates no overlap between the bounding boxes.\n1 indicates a perfect overlap between the bounding boxes.\n\nUsage of Shapely Library:\nShapely’s Polygon class is used to represent bounding boxes as geometric shapes, defined by their corner coordinates. The intersection area between two polygons is computed using the .intersection() method, which finds the overlapping region of the two bounding boxes. The union area is determined using the .union() method, which combines both polygons into a single shape.\n\n\nCode\n# convert yolo format to x_min, y_min, x_max, y_max format\ndef yolo_to_xyxy(yolo_bbox, image_size):\n    if len(yolo_bbox) == 5:\n        class_id, x_center, y_center, width, height = yolo_bbox\n    else:\n        x_center, y_center, width, height = yolo_bbox\n    x_min = (x_center - width / 2) * image_size\n    y_min = (y_center - height / 2) * image_size\n    x_max = (x_center + width / 2) * image_size\n    y_max = (y_center + height / 2) * image_size\n    return x_min, y_min, x_max, y_max\n\n# calculate iou using shapely\ndef iou_shapely(yolo_bbox1, yolo_bbox2, image_size):\n    x_min1, y_min1, x_max1, y_max1 = yolo_to_xyxy(yolo_bbox1, image_size)\n    x_min2, y_min2, x_max2, y_max2 = yolo_to_xyxy(yolo_bbox2, image_size)\n\n    polygon1 = sg.Polygon([(x_min1, y_min1), (x_max1, y_min1), (x_max1, y_max1), (x_min1, y_max1)])\n    polygon2 = sg.Polygon([(x_min2, y_min2), (x_max2, y_min2), (x_max2, y_max2), (x_min2, y_max2)])\n\n    intersection_area = polygon1.intersection(polygon2).area\n    union_area = polygon1.union(polygon2).area\n\n    if union_area == 0.0:\n        iou = 0.0\n    else:\n        iou = intersection_area / union_area\n    return iou\n\n# calculate iou using supervision\ndef iou_supervision(yolo_bbox1, yolo_bbox2, image_size):\n    box1 = np.array([yolo_to_xyxy(yolo_bbox1, image_size)])\n    box2 = np.array([yolo_to_xyxy(yolo_bbox2, image_size)])\n    iou_matrix = sv.box_iou_batch(box1, box2)\n    return iou_matrix[0, 0]\n\n\n\n\nCode\n# Example usage on YOLO bounding boxes (class_id, x_center, y_center, width, height) \nyolo_bbox1 = [0, 0.1, 0.1, 0.6, 0.6]\nyolo_bbox2 = [0, 0.3, 0.3, 0.6, 0.6]\n\niou1 = iou_shapely(yolo_bbox1, yolo_bbox2, image_size)\niou2 = iou_supervision(yolo_bbox1, yolo_bbox2, image_size)\n\nprint(f'IoU computed using shapely: {iou1:.4f}')\nprint(f'IoU computed using supervision: {iou2:.4f}')\n\n\nIoU computed using shapely: 0.2857\nIoU computed using supervision: 0.2857\n\n\n\n\n\nAverage Precision (AP) is a metric used to evaluate the performance of object detection models. It summarizes the precision-recall curve into a single value, representing the average of precision values at different recall levels.\n\nPrecision: The ratio of true positive detections to the total number of positive detections (true positives + false positives).\n\\[\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\nRecall: The ratio of true positive detections to the total number of actual positives (true positives + false negatives).\n\\[\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\n\nThe precision-recall curve is plotted with precision on the y-axis and recall on the x-axis. The Average Precision (AP) is calculated as the area under the precision-recall curve.\nThere are different methods to compute AP:\n\nPascal VOC 11-point interpolation: Precision is sampled at 11 recall levels (0.0, 0.1, …, 1.0), and the average of these precision values is taken.\nCOCO 101-point interpolation: Precision is sampled at 101 recall levels (0.0, 0.01, …, 1.0), and the average of these precision values is taken.\nArea under Precision-Recall Curve (PRC): The area under the precision-recall curve is computed using numerical integration methods.\n\n\n\nCode\nfrom sklearn.metrics import auc\n\ndef compute_ap(precisions, recalls, method=\"voc11\"):\n    # Pascal VOC 11-point method\n    if method == \"voc11\":\n        recall_points = np.linspace(0, 1, 11)\n        ap = 0.0\n        for r in recall_points:\n            ap += max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0\n        ap /= 11\n    # COCO 101-point method\n    elif method == \"coco101\":\n        recall_points = np.linspace(0, 1, 101)\n        ap = np.mean([max(precisions[recalls &gt;= r]) if np.any(recalls &gt;= r) else 0 for r in recall_points])\n    # AUC method\n    elif method == \"auc_pr\":\n        ap = auc(recalls, precisions)\n    else:\n        raise ValueError(\"Invalid AP computation method.\")\n    return ap\n\n\n# calculate precision and recall values\ndef compute_precision_recall(gt_boxes, pred_boxes, scores, image_size, iou_threshold=0.5):\n    all_tp = []\n    all_fp = []\n    num_gt_boxes = sum(len(gt) for gt in gt_boxes)\n    if num_gt_boxes == 0:\n        return np.array([]), np.array([])\n\n    for gt_boxes_image, pred_boxes_image, scores_image in zip(gt_boxes, pred_boxes, scores):\n        # Sort predicted boxes by confidence score in descending order\n        sorted_indices = np.argsort(scores_image)[::-1]\n        pred_boxes_image = [pred_boxes_image[idx] for idx in sorted_indices]\n        scores_image = [scores_image[idx] for idx in sorted_indices]\n\n        tp = np.zeros(len(pred_boxes_image))\n        fp = np.zeros(len(pred_boxes_image))\n        gt_matched = np.zeros(len(gt_boxes_image))\n\n        for j, pred_box in enumerate(pred_boxes_image):\n            if len(gt_boxes_image) == 0:\n                fp[j] = 1\n                continue\n            \n            ious = [iou_shapely(pred_box, gt_box, image_size) for gt_box in gt_boxes_image]\n            max_iou = max(ious) if ious else 0\n            max_iou_idx = np.argmax(ious) if ious else -1\n\n            if max_iou &gt;= iou_threshold and max_iou_idx != -1 and gt_matched[max_iou_idx] == 0:\n                tp[j] = 1\n                gt_matched[max_iou_idx] = 1\n            else:\n                fp[j] = 1\n\n        all_tp.extend(tp)\n        all_fp.extend(fp)\n\n    # Compute Precision-Recall\n    tp_cumsum = np.cumsum(all_tp)\n    fp_cumsum = np.cumsum(all_fp)\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)  # Avoid division by zero\n    recalls = tp_cumsum / num_gt_boxes\n\n    return precisions, recalls\n\n\n\n\n\n\n\nCode\n# generate random data\ndef generate_random_data(num_images, image_size, num_gt_boxes, num_pred_boxes, box_size):\n    images = []\n    gt_boxes = []\n    pred_boxes = []\n    scores = []\n\n    for _ in range(num_images):\n        image = np.random.rand(image_size, image_size)\n        images.append(image)\n\n        gt_boxes_image = []\n        for _ in range(num_gt_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            gt_boxes_image.append([x_min, y_min, x_max, y_max])\n        gt_boxes.append(gt_boxes_image)\n\n        pred_boxes_image = []\n        scores_image = []\n        for _ in range(num_pred_boxes):\n            x_min = np.random.randint(0, image_size - box_size)\n            y_min = np.random.randint(0, image_size - box_size)\n            x_max = x_min + box_size\n            y_max = y_min + box_size\n            pred_boxes_image.append([x_min, y_min, x_max, y_max])\n            scores_image.append(np.random.uniform(0.5, 1.0))\n        pred_boxes.append(pred_boxes_image)\n        scores.append(scores_image)\n\n    return images, gt_boxes, pred_boxes, scores\n\n\n\n\nCode\nrandom_num_images = 10\nrandom_image_size = 100\nrandom_num_gt_boxes = 10\nrandom_num_pred_boxes = 10\nrandom_box_size = 20\nrandom_images, random_gt_boxes, random_pred_boxes, random_scores = generate_random_data(random_num_images, random_image_size, random_num_gt_boxes, random_num_pred_boxes, random_box_size)\n\n\n\n\nCode\nprecisions, recalls = compute_precision_recall(random_gt_boxes, random_pred_boxes, random_scores, random_image_size, iou_threshold=0.5)\n\n# plot precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recalls, precisions, color='blue', linestyle='-', linewidth=2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute AP50\nap50_voc11 = compute_ap(precisions, recalls, method=\"voc11\")\nap50_coco101 = compute_ap(precisions, recalls, method=\"coco101\")\nap50_auc_pr = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f'AP50 using VOC11 method: {ap50_voc11:.4f}')\nprint(f'AP50 using COCO101 method: {ap50_coco101:.4f}')\nprint(f'AP50 using AUC-PR method: {ap50_auc_pr:.4f}')\n\n\nAP50 using VOC11 method: 0.2685\nAP50 using COCO101 method: 0.2571\nAP50 using AUC-PR method: 0.2188"
  },
  {
    "objectID": "posts/solar_panel_detector/solar_panel_detector.html#model-building-and-evaluation",
    "href": "posts/solar_panel_detector/solar_panel_detector.html#model-building-and-evaluation",
    "title": "Building a Solar Panel Detector",
    "section": "",
    "text": "We will split the data into training and testing sets using an 80-20 split. We will further split the training data into training and validation sets using a 90-10 split. This will allow us to train the model on the training set and tune the hyperparameters on the validation set. The testing set will be used to evaluate the model’s performance on unseen data.\n\n\nCode\nimage_dir = 'data/images_native/'\nlabel_dir = 'data/labels_native/'\nimage_size = 416\nmeters_per_pixel = 0.31  # meters per pixel\n\n\n\n\nCode\ndef split_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1):\n    os.makedirs(save_dir, exist_ok=True)\n    for split in [\"train\", \"val\", \"test\"]:\n        os.makedirs(os.path.join(save_dir, split, \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(save_dir, split, \"labels\"), exist_ok=True)\n    \n    image_names = sorted([image_name for image_name in os.listdir(image_dir) if image_name.endswith('.tif')])\n    label_names = sorted([label_name for label_name in os.listdir(label_dir) if label_name.endswith('.txt')])\n    num_images = len(image_names)\n\n    # shuffle the data\n    indices = np.arange(num_images)\n    np.random.shuffle(indices)\n\n    train_size = int(train_ratio * num_images)\n    val_size = int(val_ratio * train_size)\n    test_size = num_images - train_size\n\n    train_indices = indices[:train_size]\n    val_indices = train_indices[-val_size:]\n    train_indices = train_indices[:-val_size]\n    test_indices = indices[train_size:]\n\n    def copy_data(indices, split):\n        for idx in indices:\n            image_src = os.path.join(image_dir, image_names[idx])\n            label_src = os.path.join(label_dir, label_names[idx])\n\n            image_dst = os.path.join(save_dir, split, \"images\", image_names[idx])\n            label_dst = os.path.join(save_dir, split, \"labels\", label_names[idx])\n\n            shutil.copy(image_src, image_dst)\n            shutil.copy(label_src, label_dst)\n\n    copy_data(train_indices, \"train\")\n    copy_data(val_indices, \"val\")\n    copy_data(test_indices, \"test\")\n\n    print(f\"Data split and saved in '{save_dir}/' successfully!\")\n    print(f\"Train:      {len(train_indices)} images\")\n    print(f\"Validation: {len(val_indices)} images\")\n    print(f\"Test:       {len(test_indices)} images\")\n\n\n\nsplit_data(image_dir, label_dir, save_dir=\"split_data\", train_ratio=0.8, val_ratio=0.1)\n\n\nData split and saved in 'split_data/' successfully!\nTrain:      1830 images\nValidation: 203 images\nTest:       509 images\n\n\n\n\nCode\n# Create YAML content with absolute paths\nbase_path = os.path.abspath('./split_data')\nyaml_content = f\"\"\"path: {base_path}\ntrain: {os.path.join(base_path, 'train', 'images')}\nval: {os.path.join(base_path, 'val', 'images')}\ntest: {os.path.join(base_path, 'test', 'images')}\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\"\"\"\ndata_yml_path = os.path.join(base_path, 'data.yaml')\nwith open(data_yml_path, 'w') as f:\n    f.write(yaml_content)\nprint(f\"data.yaml created at:\\n{data_yml_path}\")\ndel base_path\n\n\ndata.yaml created at:\nc:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\data.yaml\n\n\n\n\nCode\nprint(yaml_content)\n\n\npath: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\ntrain: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\train\\images\nval: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\val\\images\ntest: c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\n\nnc: 1  # number of classes\nnames: ['solar_panel']  # class names\n\n\n\n\n\n\n\n\nCode\nmodel = YOLO(\"yolo12x.pt\")\n\nresults = model.train(\n    data=\"split_data/data.yaml\",\n    epochs=100,\n    imgsz=416,\n    batch=100,\n    device=device,\n    project=\"models\",\n    name=\"yolo12x\",\n)\n\n\n\n\nCode\n# load the best model\nmodel = YOLO(\"models/yolo12x/weights/best.pt\")\n\n\n\n\nCode\ndf = pd.read_csv('models/yolo12x/results.csv')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# First subplot: Training loss\nax1.plot(df['epoch'], df['train/box_loss'], label='Box Loss')\nax1.plot(df['epoch'], df['train/cls_loss'], label='Cls Loss')\nax1.plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss')\ndf['train/total_loss'] = df['train/box_loss'] + df['train/cls_loss'] + df['train/dfl_loss']\nax1.plot(df['epoch'], df['train/total_loss'], label='Total Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training Loss')\nax1.legend()\nax1.grid(True)\n\n# Second subplot: Validation loss\nax2.plot(df['epoch'], df['val/box_loss'], label='Box Loss')\nax2.plot(df['epoch'], df['val/cls_loss'], label='Cls Loss')\nax2.plot(df['epoch'], df['val/dfl_loss'], label='DFL Loss')\ndf['val/total_loss'] = df['val/box_loss'] + df['val/cls_loss'] + df['val/dfl_loss']\nax2.plot(df['epoch'], df['val/total_loss'], label='Total Loss')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss')\nax2.set_title('Validation Loss')\nax2.legend()\nax2.set_ylim(ax1.get_ylim())\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load the test data\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\n# Randomly select 3-4 images\nnum_images = 4\nrandom_images = np.random.choice(test_images, num_images, replace=False)\n\nfig, axes = plt.subplots(num_images, 2, figsize=(15, 5 * num_images))\n\nfor i, image_name in enumerate(random_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Predict using the trained model\n    results = model(image_path)[0]  # Get the first result object\n\n    # Load the image\n    image = plt.imread(image_path)\n\n    # Plot the ground truth labels (green)\n    with open(label_path, 'r') as f:\n        gt_labels = f.readlines()\n    axes[i, 0].imshow(image)\n    axes[i, 0].axis('off')\n    axes[i, 0].set_title(f\"Ground Truth: {image_name}\")\n    for gt_label in gt_labels:\n        class_id, x_center, y_center, width, height = map(float, gt_label.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='green', facecolor='none')\n        axes[i, 0].add_patch(rect)\n        axes[i, 0].text(x_min, y_min - 5, 'Ground Truth', color='green', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\n    # Plot the predicted labels (red)\n    axes[i, 1].imshow(image)\n    axes[i, 1].axis('off')\n    axes[i, 1].set_title(f\"Predictions: {image_name}\")\n    boxes = results.boxes\n    for box in boxes:\n        x_min, y_min, x_max, y_max = box.xyxy[0].cpu().numpy()\n        conf = float(box.conf[0])\n        cls = int(box.cls[0])\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='blue', facecolor='none')\n        axes[i, 1].add_patch(rect)\n        axes[i, 1].text(x_min, y_max + 15, f'Pred: {conf:.2f}', color='blue', fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\n# save figure\nfig.savefig('predictions.png')\n\n\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3870_y0_11610_dxdy_416.tif: 416x416 19 solar_panels, 124.0ms\nSpeed: 2.3ms preprocess, 124.0ms inference, 4.5ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_1__x0_3061_y0_8047_dxdy_416.tif: 416x416 5 solar_panels, 84.0ms\nSpeed: 2.5ms preprocess, 84.0ms inference, 4.9ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_7463_y0_9890_dxdy_416.tif: 416x416 8 solar_panels, 91.4ms\nSpeed: 1.7ms preprocess, 91.4ms inference, 4.3ms postprocess per image at shape (1, 3, 416, 416)\n\nimage 1/1 c:\\Users\\shard\\Desktop\\SRIP-Project-Task\\split_data\\test\\images\\solarpanels_native_3__x0_8067_y0_10432_dxdy_416.tif: 416x416 2 solar_panels, 69.2ms\nSpeed: 1.6ms preprocess, 69.2ms inference, 4.6ms postprocess per image at shape (1, 3, 416, 416)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef load_detections(image_path):\n    label_path = image_path.replace('images', 'labels').replace('.tif', '.txt')\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    \n    xyxy_list = []\n    class_ids = []\n    scores = []\n\n    for line in lines:\n        class_id, x_center, y_center, width, height = map(float, line.split())\n        x_min, y_min, x_max, y_max = yolo_to_xyxy([x_center, y_center, width, height], image_size)\n        xyxy_list.append([x_min, y_min, x_max, y_max])\n        class_ids.append(class_id)\n        scores.append(1.0)\n\n    detections = sv.Detections(\n        xyxy=np.array(xyxy_list),\n        class_id=np.array(class_ids),\n        confidence=np.array(scores),\n        metadata={\"image_name\": os.path.basename(image_path)}\n    )\n\n    return detections\n\n\n\n\nCode\n# load the ground truth and predictions\ntest_images = os.listdir('split_data/test/images')\ntest_labels = os.listdir('split_data/test/labels')\n\ntargets = []\npredictions = []\n\nfor i, image_name in enumerate(test_images):\n    image_path = os.path.join('split_data/test/images', image_name)\n    label_path = os.path.join('split_data/test/labels', image_name.replace('.tif', '.txt'))\n\n    # Load the ground truth and predictions\n    target_sv_detection = load_detections(image_path)\n    # print(target_sv_detection.xyxy)\n\n    results = model(image_path, verbose=False)[0] \n    pred_sv_detection = sv.Detections.from_ultralytics(results)\n    # print(pred_sv_detection.xyxy)\n\n    targets.append(target_sv_detection)\n    predictions.append(pred_sv_detection)\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\nimport pickle\n\n# Save the detections\nwith open(\"detections.pkl\", \"wb\") as f:\n    pickle.dump({\"targets\": targets, \"predictions\": predictions}, f)\n\n# Load the detections\nwith open(\"detections.pkl\", \"rb\") as f:\n    data = pickle.load(f)\ntargets = data[\"targets\"]\npredictions = data[\"predictions\"]\n\nprint(len(targets), len(predictions))\n\n\n509 509\n\n\n\n\nCode\n# Compute mAP50 using supervision\nmetrics = sv.metrics.MeanAveragePrecision()\nresults = metrics.update(targets, predictions).compute()\nmAP50_supervision = results.map50\nprint(f\"mAP50 using supervision: {mAP50_supervision:.4f}\")\n\n\nmAP50 using supervision: 0.9018\n\n\n\n\nCode\n# Compute mAP50 using my implementation\nall_gt_boxes, all_pred_boxes, all_scores = [], [], []\n\nfor target, prediction in zip(targets, predictions):\n    all_gt_boxes.append(target.xyxy)\n    all_pred_boxes.append(prediction.xyxy)\n    all_scores.append(prediction.confidence)\n\nprecisions, recalls = compute_precision_recall(all_gt_boxes, all_pred_boxes, all_scores, image_size, iou_threshold=0.5)\nmAP50_mine = compute_ap(precisions, recalls, method=\"auc_pr\")\n\nprint(f\"mAP50 using my implementation: {mAP50_mine:.4f}\")\n\n\nmAP50 using my implementation: 0.8930\n\n\nTODO:\nCreate a table of Precision, Recall and F1-scores where rows are IoU thresholds [0.1, 0.3, 0.5, 0.7, 0.9] and columns are confidence thresholds [0.1, 0.3, 0.5, 0.7, 0.9]\n(Hint use supervision.metrics.ConfusionMatrix to get the confusion matrix and get TP, FP and FN from it to compute the P, R and F-1)"
  }
]